{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2024-04-24T09:03:53.563484Z","iopub.status.busy":"2024-04-24T09:03:53.563116Z","iopub.status.idle":"2024-04-24T09:04:00.431950Z","shell.execute_reply":"2024-04-24T09:04:00.431129Z","shell.execute_reply.started":"2024-04-24T09:03:53.563457Z"},"trusted":true},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","import re\n","from nltk.tokenize import word_tokenize\n","\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.utils.data import DataLoader, TensorDataset\n","from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n","from tqdm import tqdm\n","\n","UNKNOWN_TOKEN='UNK'\n","PAD_TOKEN='PAD'\n","START_TOKEN = '<START>'\n","END_TOKEN = '<END>'\n"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2024-04-24T09:04:09.792745Z","iopub.status.busy":"2024-04-24T09:04:09.792248Z","iopub.status.idle":"2024-04-24T09:04:09.824546Z","shell.execute_reply":"2024-04-24T09:04:09.823605Z","shell.execute_reply.started":"2024-04-24T09:04:09.792717Z"},"trusted":true},"outputs":[{"data":{"text/plain":["device(type='cuda')"]},"execution_count":2,"metadata":{},"output_type":"execute_result"}],"source":["device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","device"]},{"cell_type":"markdown","metadata":{},"source":["## Data"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2024-04-24T09:04:12.017911Z","iopub.status.busy":"2024-04-24T09:04:12.017572Z","iopub.status.idle":"2024-04-24T09:04:12.635296Z","shell.execute_reply":"2024-04-24T09:04:12.634411Z","shell.execute_reply.started":"2024-04-24T09:04:12.017885Z"},"trusted":true},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Class Index</th>\n","      <th>Description</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>3</td>\n","      <td>Reuters - Short-sellers, Wall Street's dwindli...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>3</td>\n","      <td>Reuters - Private investment firm Carlyle Grou...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>3</td>\n","      <td>Reuters - Soaring crude prices plus worries\\ab...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>3</td>\n","      <td>Reuters - Authorities have halted oil export\\f...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>3</td>\n","      <td>AFP - Tearaway world oil prices, toppling reco...</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>119995</th>\n","      <td>1</td>\n","      <td>KARACHI (Reuters) - Pakistani President Perve...</td>\n","    </tr>\n","    <tr>\n","      <th>119996</th>\n","      <td>2</td>\n","      <td>Red Sox general manager Theo Epstein acknowled...</td>\n","    </tr>\n","    <tr>\n","      <th>119997</th>\n","      <td>2</td>\n","      <td>The Miami Dolphins will put their courtship of...</td>\n","    </tr>\n","    <tr>\n","      <th>119998</th>\n","      <td>2</td>\n","      <td>PITTSBURGH at NY GIANTS Time: 1:30 p.m. Line: ...</td>\n","    </tr>\n","    <tr>\n","      <th>119999</th>\n","      <td>2</td>\n","      <td>INDIANAPOLIS -- All-Star Vince Carter was trad...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>120000 rows × 2 columns</p>\n","</div>"],"text/plain":["        Class Index                                        Description\n","0                 3  Reuters - Short-sellers, Wall Street's dwindli...\n","1                 3  Reuters - Private investment firm Carlyle Grou...\n","2                 3  Reuters - Soaring crude prices plus worries\\ab...\n","3                 3  Reuters - Authorities have halted oil export\\f...\n","4                 3  AFP - Tearaway world oil prices, toppling reco...\n","...             ...                                                ...\n","119995            1   KARACHI (Reuters) - Pakistani President Perve...\n","119996            2  Red Sox general manager Theo Epstein acknowled...\n","119997            2  The Miami Dolphins will put their courtship of...\n","119998            2  PITTSBURGH at NY GIANTS Time: 1:30 p.m. Line: ...\n","119999            2  INDIANAPOLIS -- All-Star Vince Carter was trad...\n","\n","[120000 rows x 2 columns]"]},"execution_count":3,"metadata":{},"output_type":"execute_result"}],"source":["train_data=pd.read_csv('/kaggle/input/assignment-3/train.csv')\n","test_data=pd.read_csv('/kaggle/input/assignment-3/test.csv')\n","\n","train_data"]},{"cell_type":"markdown","metadata":{},"source":["## Data Preprocessing"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2024-04-24T09:04:14.836002Z","iopub.status.busy":"2024-04-24T09:04:14.835317Z","iopub.status.idle":"2024-04-24T09:04:14.845030Z","shell.execute_reply":"2024-04-24T09:04:14.844071Z","shell.execute_reply.started":"2024-04-24T09:04:14.835972Z"},"trusted":true},"outputs":[],"source":["def preprocess_text(data,type='train'):\n","    sentences=[]\n","    vocab=set()\n","    vocab.add(PAD_TOKEN)\n","    vocab.add(UNKNOWN_TOKEN)\n","    total=0\n","\n","    frequency=dict()\n","    for text in data:\n","        text = re.sub(r'[^\\w\\s\\n]', ' ', str(text).lower())\n","        words = word_tokenize(text)\n","        words = [START_TOKEN] + words + [END_TOKEN]\n","        sentences.append(words)\n","        for word in words:\n","            frequency[word]=frequency.get(word,0)+1\n","            total+=1\n","    \n","    if type=='train':\n","        frequency_threshold=3\n","        for i in range(len(sentences)):\n","            for j in range(len(sentences[i])):\n","                if frequency[sentences[i][j]]<frequency_threshold:\n","                    sentences[i][j]=UNKNOWN_TOKEN\n","\n","    for sentence in sentences:\n","        for word in sentence:\n","            vocab.add(word)\n","    vocab=list(vocab)\n","    vocab = sorted(vocab)\n","    return sentences,vocab"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2024-04-24T09:04:17.680788Z","iopub.status.busy":"2024-04-24T09:04:17.680434Z","iopub.status.idle":"2024-04-24T09:05:05.850415Z","shell.execute_reply":"2024-04-24T09:05:05.849403Z","shell.execute_reply.started":"2024-04-24T09:04:17.680758Z"},"trusted":true},"outputs":[],"source":["sentences_train,vocab = preprocess_text(train_data['Description'])\n","sentences_test,_ = preprocess_text(test_data['Description'],'test')"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2024-04-24T09:05:05.852309Z","iopub.status.busy":"2024-04-24T09:05:05.852007Z","iopub.status.idle":"2024-04-24T09:05:05.857427Z","shell.execute_reply":"2024-04-24T09:05:05.856389Z","shell.execute_reply.started":"2024-04-24T09:05:05.852283Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["32009\n"]}],"source":["print(len(vocab))"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2024-04-24T09:05:05.859135Z","iopub.status.busy":"2024-04-24T09:05:05.858787Z","iopub.status.idle":"2024-04-24T09:05:05.888470Z","shell.execute_reply":"2024-04-24T09:05:05.887696Z","shell.execute_reply.started":"2024-04-24T09:05:05.859103Z"},"trusted":true},"outputs":[],"source":["word2id = {}\n","id2word = {}\n","sorted_vocab = sorted(vocab)\n","for i, word in enumerate(sorted_vocab):\n","    word2id[word] = i\n","    id2word[i] = word"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2024-04-24T09:05:10.805947Z","iopub.status.busy":"2024-04-24T09:05:10.805647Z","iopub.status.idle":"2024-04-24T09:05:12.039163Z","shell.execute_reply":"2024-04-24T09:05:12.038388Z","shell.execute_reply.started":"2024-04-24T09:05:10.805920Z"},"trusted":true},"outputs":[],"source":["sentence_lengths = [len(sentence) for sentence in sentences_train]\n","sorted_lengths = sorted(sentence_lengths)\n","index_95th_percentile = int(np.percentile(range(len(sorted_lengths)), 95))\n","length_95th_percentile = sorted_lengths[index_95th_percentile]\n","length_sentence=length_95th_percentile\n","\n","for i in range(len(sentences_train)):\n","    sentence=sentences_train[i]\n","    sentences_train[i]=[word2id.get(word,word2id[UNKNOWN_TOKEN]) for word in sentence]\n","for i in range(len(sentences_test)):\n","    sentence=sentences_test[i]\n","    sentences_test[i]=[word2id.get(word,word2id[UNKNOWN_TOKEN]) for word in sentence]"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2024-04-24T09:05:05.890675Z","iopub.status.busy":"2024-04-24T09:05:05.890361Z","iopub.status.idle":"2024-04-24T09:05:10.804561Z","shell.execute_reply":"2024-04-24T09:05:10.803720Z","shell.execute_reply.started":"2024-04-24T09:05:05.890651Z"},"trusted":true},"outputs":[],"source":["def load_glove_model(file_path):\n","    word_vectors = {}\n","\n","    with open(file_path, 'r', encoding='utf-8') as f:\n","        for line in f:\n","            values = line.split()\n","            word = values[0]\n","            vector = [float(val) for val in values[1:]]\n","            word_vectors[word] = vector\n","\n","    return word_vectors\n","\n","glove_file_path = '/kaggle/input/gloveembeddings/glove.6B.100d.txt'\n","glove_dict = load_glove_model(glove_file_path)"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2024-04-24T09:05:12.040543Z","iopub.status.busy":"2024-04-24T09:05:12.040237Z","iopub.status.idle":"2024-04-24T09:05:12.045762Z","shell.execute_reply":"2024-04-24T09:05:12.044875Z","shell.execute_reply.started":"2024-04-24T09:05:12.040517Z"},"trusted":true},"outputs":[],"source":["def create_embedding_matrix(glove_dict):\n","    weights_matrix = torch.zeros((len(vocab), 100))\n","    for i, word in enumerate(vocab):\n","        weights_matrix[i] = torch.tensor(glove_dict.get(word,np.random.uniform(-1, 1, size=100)))\n","    return weights_matrix\n"]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2024-04-24T09:05:12.047291Z","iopub.status.busy":"2024-04-24T09:05:12.046953Z","iopub.status.idle":"2024-04-24T09:05:13.672547Z","shell.execute_reply":"2024-04-24T09:05:13.671562Z","shell.execute_reply.started":"2024-04-24T09:05:12.047259Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["torch.Size([32009, 100])\n"]}],"source":["embedding_matrix = create_embedding_matrix(glove_dict)\n","print(embedding_matrix.size())\n"]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2024-04-24T09:05:13.674101Z","iopub.status.busy":"2024-04-24T09:05:13.673783Z","iopub.status.idle":"2024-04-24T09:05:13.797933Z","shell.execute_reply":"2024-04-24T09:05:13.796806Z","shell.execute_reply.started":"2024-04-24T09:05:13.674074Z"},"trusted":true},"outputs":[],"source":["class ELMo(nn.Module):\n","    def __init__(self, vocab_size, embedding_dim, hidden_dim, embedding_matrix):\n","        super(ELMo, self).__init__()\n","        self.vocab_size = vocab_size\n","        self.embedding_dim = embedding_dim\n","        self.hidden_dim = hidden_dim\n","        self.embedding1 = nn.Embedding.from_pretrained(embedding_matrix)\n","        self.embedding2 = nn.Embedding.from_pretrained(embedding_matrix)\n","        self.lstm_forward1 = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)\n","        self.lstm_forward2 = nn.LSTM(hidden_dim, hidden_dim, batch_first=True)\n","        self.lstm_backward1 = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)\n","        self.lstm_backward2 = nn.LSTM(hidden_dim, hidden_dim, batch_first=True)\n","        self.linear_mode1 = nn.Linear(200, vocab_size)\n","        self.linear_mode2 = nn.Linear(200, vocab_size)\n","\n","    def forward(self, input_data, mode):\n","        if mode == 1:\n","            forward_embed = self.embedding1(input_data)\n","            forward_lstm1, _ = self.lstm_forward1(forward_embed) \n","            forward_lstm2, _ = self.lstm_forward2(forward_lstm1) \n","            lstm_concat = torch.cat((forward_lstm1, forward_lstm2), dim=-1)\n","            output = self.linear_mode1(lstm_concat)\n","            return output\n","        \n","        elif mode == 2:\n","            backward_embed = self.embedding2(input_data)\n","            backward_lstm1, _ = self.lstm_backward1(backward_embed) \n","            backward_lstm2, _ = self.lstm_backward2(backward_lstm1) \n","            lstm_concat = torch.cat((backward_lstm1, backward_lstm2), dim=-1)\n","            output = self.linear_mode2(lstm_concat)\n","            return output\n"]},{"cell_type":"code","execution_count":17,"metadata":{"execution":{"iopub.execute_input":"2024-04-24T06:21:13.876557Z","iopub.status.busy":"2024-04-24T06:21:13.876172Z","iopub.status.idle":"2024-04-24T06:21:14.047040Z","shell.execute_reply":"2024-04-24T06:21:14.046144Z","shell.execute_reply.started":"2024-04-24T06:21:13.876521Z"},"trusted":true},"outputs":[{"data":{"text/plain":["ELMo(\n","  (embedding1): Embedding(32009, 100)\n","  (embedding2): Embedding(32009, 100)\n","  (lstm_forward1): LSTM(100, 100, batch_first=True)\n","  (lstm_forward2): LSTM(100, 100, batch_first=True)\n","  (lstm_backward1): LSTM(100, 100, batch_first=True)\n","  (lstm_backward2): LSTM(100, 100, batch_first=True)\n","  (linear_mode1): Linear(in_features=200, out_features=32009, bias=True)\n","  (linear_mode2): Linear(in_features=200, out_features=32009, bias=True)\n",")"]},"execution_count":17,"metadata":{},"output_type":"execute_result"}],"source":["vocab_size = len(vocab)\n","embedding_dim = 100\n","hidden_dim = 100\n","batch_size=32\n","\n","elmo = ELMo(vocab_size, embedding_dim, hidden_dim, embedding_matrix)\n","elmo.to(device)"]},{"cell_type":"code","execution_count":18,"metadata":{"execution":{"iopub.execute_input":"2024-04-24T06:21:14.051033Z","iopub.status.busy":"2024-04-24T06:21:14.050769Z","iopub.status.idle":"2024-04-24T06:21:17.958870Z","shell.execute_reply":"2024-04-24T06:21:17.957843Z","shell.execute_reply.started":"2024-04-24T06:21:14.051011Z"},"trusted":true},"outputs":[],"source":["X_train = []\n","for sentence in sentences_train:\n","    if len(sentence) < length_sentence:\n","        padding_needed = length_sentence - len(sentence)\n","        sentence.extend(padding_needed*[word2id[PAD_TOKEN]])\n","    X_train.append(torch.tensor(sentence[:length_sentence]))\n","y_train = pd.get_dummies(train_data['Class Index'], prefix='value', dtype=int).values\n","\n","X_test = []\n","for sentence in sentences_test:\n","    if len(sentence) < length_sentence:\n","        padding_needed = length_sentence - len(sentence)\n","        sentence.extend(padding_needed*[word2id[PAD_TOKEN]])\n","    X_test.append(torch.tensor(sentence[:length_sentence]))\n","y_test = pd.get_dummies(test_data['Class Index'], prefix='value', dtype=int).values\n","\n","\n","X_train_tensor = torch.stack(X_train)\n","X_test_tensor = torch.stack(X_test)\n","y_train_tensor = torch.tensor(y_train,dtype=torch.float32)\n","y_test_tensor = torch.tensor(y_test,dtype=torch.float32)\n","\n","train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n","train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n","test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n","test_loader = DataLoader(test_dataset, batch_size=32, shuffle=True)\n"]},{"cell_type":"code","execution_count":19,"metadata":{"execution":{"iopub.execute_input":"2024-04-24T06:21:17.960534Z","iopub.status.busy":"2024-04-24T06:21:17.960162Z","iopub.status.idle":"2024-04-24T07:08:44.230168Z","shell.execute_reply":"2024-04-24T07:08:44.229143Z","shell.execute_reply.started":"2024-04-24T06:21:17.960509Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["100%|██████████| 3750/3750 [02:22<00:00, 26.26it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 1/10 (Mode 1) - Train Loss: 0.0036\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 3750/3750 [02:22<00:00, 26.37it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 2/10 (Mode 1) - Train Loss: 0.0031\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 3750/3750 [02:22<00:00, 26.40it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 3/10 (Mode 1) - Train Loss: 0.0029\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 3750/3750 [02:22<00:00, 26.39it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 4/10 (Mode 1) - Train Loss: 0.0028\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 3750/3750 [02:22<00:00, 26.40it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 5/10 (Mode 1) - Train Loss: 0.0027\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 3750/3750 [02:21<00:00, 26.42it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 6/10 (Mode 1) - Train Loss: 0.0027\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 3750/3750 [02:22<00:00, 26.39it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 7/10 (Mode 1) - Train Loss: 0.0026\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 3750/3750 [02:21<00:00, 26.42it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 8/10 (Mode 1) - Train Loss: 0.0026\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 3750/3750 [02:21<00:00, 26.43it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 9/10 (Mode 1) - Train Loss: 0.0026\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 3750/3750 [02:21<00:00, 26.42it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 10/10 (Mode 1) - Train Loss: 0.0025\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 3750/3750 [02:22<00:00, 26.40it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 1/10 (Mode 2) - Train Loss: 0.0033\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 3750/3750 [02:22<00:00, 26.33it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 2/10 (Mode 2) - Train Loss: 0.0029\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 3750/3750 [02:22<00:00, 26.31it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 3/10 (Mode 2) - Train Loss: 0.0028\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 3750/3750 [02:22<00:00, 26.32it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 4/10 (Mode 2) - Train Loss: 0.0027\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 3750/3750 [02:22<00:00, 26.31it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 5/10 (Mode 2) - Train Loss: 0.0026\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 3750/3750 [02:22<00:00, 26.32it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 6/10 (Mode 2) - Train Loss: 0.0026\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 3750/3750 [02:22<00:00, 26.39it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 7/10 (Mode 2) - Train Loss: 0.0025\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 3750/3750 [02:22<00:00, 26.39it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 8/10 (Mode 2) - Train Loss: 0.0025\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 3750/3750 [02:22<00:00, 26.40it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 9/10 (Mode 2) - Train Loss: 0.0025\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 3750/3750 [02:22<00:00, 26.39it/s]"]},{"name":"stdout","output_type":"stream","text":["Epoch 10/10 (Mode 2) - Train Loss: 0.0024\n"]},{"name":"stderr","output_type":"stream","text":["\n"]}],"source":["from tqdm import tqdm\n","\n","num_epochs = 10\n","criterion = nn.CrossEntropyLoss(ignore_index=word2id[PAD_TOKEN])\n","optimizer = optim.Adam(elmo.parameters(), lr=0.001)  \n","\n","for epoch in range(num_epochs):\n","    elmo.train()\n","    total_loss = 0.0\n","    total_tokens = 0\n","    for inputs in tqdm(train_loader):\n","        inputs = inputs[0]\n","        inputs = inputs.to(device)\n","        optimizer.zero_grad()\n","        input_seq = inputs[:, :length_sentence-1]\n","        target_seq = inputs[:, 1:]\n","        outputs = elmo(input_seq, mode=1)\n","        loss = criterion(outputs.permute(0, 2, 1), target_seq)  \n","        total_loss += loss.item()\n","        total_tokens += target_seq.numel()\n","        loss.backward()\n","        optimizer.step()\n","    avg_loss = total_loss / total_tokens\n","    print(f\"Epoch {epoch+1}/{num_epochs} (Mode 1) - Train Loss: {avg_loss:.4f}\")\n","\n","for epoch in range(num_epochs):\n","    elmo.train()\n","    total_loss = 0.0\n","    total_tokens = 0\n","    for inputs in tqdm(train_loader):\n","        inputs = inputs[0]\n","        inputs = inputs.to(device)\n","        optimizer.zero_grad()\n","        inputs = torch.flip(inputs, dims=[1])\n","        input_seq = inputs[:, :length_sentence-1]\n","        target_seq = inputs[:, 1:]\n","        outputs = elmo(input_seq, mode=1)\n","        loss = criterion(outputs.permute(0, 2, 1), target_seq) \n","        total_loss += loss.item()\n","        total_tokens += target_seq.numel()\n","        loss.backward()\n","        optimizer.step()\n","    avg_loss = total_loss / total_tokens\n","    print(f\"Epoch {epoch+1}/{num_epochs} (Mode 2) - Train Loss: {avg_loss:.4f}\")"]},{"cell_type":"code","execution_count":20,"metadata":{"execution":{"iopub.execute_input":"2024-04-24T07:08:44.232074Z","iopub.status.busy":"2024-04-24T07:08:44.231547Z","iopub.status.idle":"2024-04-24T07:08:51.565182Z","shell.execute_reply":"2024-04-24T07:08:51.564234Z","shell.execute_reply.started":"2024-04-24T07:08:44.232048Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["100%|██████████| 238/238 [00:03<00:00, 65.07it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Test Loss (Mode 1): 0.0057\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 238/238 [00:03<00:00, 65.03it/s]"]},{"name":"stdout","output_type":"stream","text":["Test Loss (Mode 2): 0.0062\n"]},{"name":"stderr","output_type":"stream","text":["\n"]}],"source":["elmo.eval()\n","total_loss = 0.0\n","total_tokens = 0 \n","with torch.no_grad():\n","    for inputs in tqdm(test_loader):\n","        inputs = inputs[0]\n","        inputs = inputs.to(device)\n","        input_seq = inputs[:, :length_sentence-1]\n","        target_seq = inputs[:, 1:]\n","        outputs = elmo(input_seq, mode=1)\n","        loss = criterion(outputs.permute(0, 2, 1), target_seq)  \n","        total_loss += loss.item()\n","        total_tokens += target_seq.numel()\n","    avg_loss = total_loss / total_tokens\n","print(f\"Test Loss (Mode 1): {avg_loss:.4f}\")\n","\n","elmo.eval()\n","total_loss = 0.0\n","total_tokens = 0  \n","with torch.no_grad():\n","    for inputs in tqdm(test_loader):\n","        inputs = inputs[0]\n","        inputs = inputs.to(device)\n","        inputs = torch.flip(inputs, dims=[1])\n","        input_seq = inputs[:, :length_sentence-1]\n","        target_seq = inputs[:, 1:]\n","        outputs = elmo(input_seq, mode=2)\n","        loss = criterion(outputs.permute(0, 2, 1), target_seq) \n","        total_loss += loss.item()\n","        total_tokens += target_seq.numel()\n","    avg_loss = total_loss / total_tokens\n","print(f\"Test Loss (Mode 2): {avg_loss:.4f}\")"]},{"cell_type":"code","execution_count":21,"metadata":{"execution":{"iopub.execute_input":"2024-04-24T07:08:51.566646Z","iopub.status.busy":"2024-04-24T07:08:51.566372Z","iopub.status.idle":"2024-04-24T07:08:51.716174Z","shell.execute_reply":"2024-04-24T07:08:51.715128Z","shell.execute_reply.started":"2024-04-24T07:08:51.566623Z"},"trusted":true},"outputs":[],"source":["model_path='/kaggle/working/bilstm.pth'\n","torch.save(elmo.state_dict(), model_path)"]},{"cell_type":"code","execution_count":13,"metadata":{"execution":{"iopub.execute_input":"2024-04-24T09:05:13.799496Z","iopub.status.busy":"2024-04-24T09:05:13.799164Z","iopub.status.idle":"2024-04-24T09:05:15.089757Z","shell.execute_reply":"2024-04-24T09:05:15.088806Z","shell.execute_reply.started":"2024-04-24T09:05:13.799468Z"},"trusted":true},"outputs":[{"data":{"text/plain":["<All keys matched successfully>"]},"execution_count":13,"metadata":{},"output_type":"execute_result"}],"source":["vocab_size = len(vocab)\n","embedding_dim = 100\n","hidden_dim = 100\n","batch_size=32\n","\n","model_path = \"/kaggle/input/bilstm2/bilstm (1).pth\"\n","elmo = ELMo(vocab_size, embedding_dim, hidden_dim, embedding_matrix)\n","state_dict = torch.load(model_path)\n","\n","elmo.load_state_dict(state_dict)\n"]},{"cell_type":"code","execution_count":25,"metadata":{"execution":{"iopub.execute_input":"2024-04-24T07:13:40.494069Z","iopub.status.busy":"2024-04-24T07:13:40.493736Z","iopub.status.idle":"2024-04-24T07:15:48.704169Z","shell.execute_reply":"2024-04-24T07:15:48.703186Z","shell.execute_reply.started":"2024-04-24T07:13:40.494044Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/10, Loss: 0.4964531057993571\n","Epoch 2/10, Loss: 0.26761747864931823\n","Epoch 3/10, Loss: 0.23608226016064485\n","Epoch 4/10, Loss: 0.210938997592777\n","Epoch 5/10, Loss: 0.18908926866576076\n","Epoch 6/10, Loss: 0.1695423967368901\n","Epoch 7/10, Loss: 0.1516266350803276\n","Epoch 8/10, Loss: 0.1350703932646662\n","Epoch 9/10, Loss: 0.11938067760529618\n","Epoch 10/10, Loss: 0.10560751794824998\n","Train Set:\n","Accuracy: 0.9702833333333334\n","Precision: 0.9702992419278945\n","Recall: 0.9702833333333334\n","F1 Score: 0.9702506269056209\n","Confusion Matrix: [[29281   157   358   204]\n"," [   50 29889    18    43]\n"," [  320    57 28892   731]\n"," [  483    32  1113 28372]]\n","\n","Test Set:\n","Accuracy: 0.9092105263157895\n","Precision: 0.9092638823760439\n","Recall: 0.9092105263157895\n","F1 Score: 0.9092170828865068\n","Confusion Matrix: [[1737   35   73   55]\n"," [  24 1849   13   14]\n"," [  63   11 1669  157]\n"," [  62   11  172 1655]]\n"]}],"source":["X_train = []\n","for sentence in sentences_train:\n","    sentence_embedding = [embedding_matrix[word] for word in sentence]\n","    if len(sentence) < length_sentence:\n","        padding_needed = length_sentence - len(sentence)\n","        sentence_embedding.extend(padding_needed*[embedding_matrix[word2id[PAD_TOKEN]]])\n","    if sentence_embedding:\n","        X_train.append(torch.stack(sentence_embedding[:length_sentence]))\n","y_train = pd.get_dummies(train_data['Class Index'], prefix='value', dtype=int).values\n","\n","X_test = []\n","for sentence in sentences_test:\n","    sentence_embedding = [embedding_matrix[word] for word in sentence]\n","    if len(sentence) < length_sentence:\n","        padding_needed = length_sentence - len(sentence)\n","        sentence_embedding.extend(padding_needed*[embedding_matrix[word2id[PAD_TOKEN]]])\n","    if sentence_embedding:\n","        X_test.append(torch.stack(sentence_embedding[:length_sentence]))\n","y_test = pd.get_dummies(test_data['Class Index'], prefix='value', dtype=int).values\n","\n","\n","X_train_tensor = torch.stack(X_train)\n","X_test_tensor = torch.stack(X_test)\n","y_train_tensor = torch.tensor(y_train,dtype=torch.float32)\n","y_test_tensor = torch.tensor(y_test,dtype=torch.float32)\n","\n","class LSTMModel(nn.Module):\n","    def __init__(self, input_size, hidden_size, output_size):\n","        super(LSTMModel, self).__init__()\n","        self.lstm = nn.LSTM(input_size, hidden_size, batch_first=True)\n","        self.fc = nn.Linear(hidden_size, output_size)\n","\n","    def forward(self, x):\n","        output, _ = self.lstm(x)\n","        output = self.fc(output[:, -1, :]) \n","        return output\n","\n","input_size = 100  \n","hidden_size = 128\n","output_size = 4\n","model = LSTMModel(input_size, hidden_size, output_size).to(device)\n","\n","criterion = nn.CrossEntropyLoss(ignore_index=word2id[PAD_TOKEN])\n","optimizer = optim.Adam(model.parameters(), lr=0.001)\n","\n","train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n","train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n","\n","epochs = 10\n","for epoch in range(epochs):\n","    model.train()\n","    total_loss = 0.0\n","    total_samples = 0\n","    \n","    for batch_X, batch_y in tqdm(train_loader):\n","        batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n","        optimizer.zero_grad()\n","        outputs = model(batch_X)\n","        _, target_indices = batch_y.max(dim=1)\n","        loss = criterion(outputs, target_indices)\n","        loss.backward()\n","        optimizer.step()\n","        total_loss += loss.item() * batch_X.size(0)\n","        total_samples += batch_X.size(0)\n","    epoch_loss = total_loss / total_samples\n","    print(f\"Epoch {epoch+1}/{epochs}, Loss: {epoch_loss}\")\n","\n","model.eval()\n","with torch.no_grad():\n","    y_true = []\n","    y_pred = []\n","    for batch_X, batch_y in train_loader:\n","        batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n","        outputs = model(batch_X)\n","        _, predicted = torch.max(outputs, 1)\n","        y_true.extend(torch.argmax(batch_y, dim=1).cpu().numpy())\n","        y_pred.extend(predicted.cpu().numpy())\n","    accuracy = accuracy_score(y_true, y_pred)\n","    precision = precision_score(y_true, y_pred, average='weighted')\n","    recall = recall_score(y_true, y_pred, average='weighted')\n","    f1 = f1_score(y_true, y_pred, average='weighted')\n","    cm = confusion_matrix(y_true, y_pred)\n","    print(\"Train Set:\")\n","    print(\"Accuracy:\", accuracy)\n","    print(\"Precision:\", precision)\n","    print(\"Recall:\", recall)\n","    print(\"F1 Score:\", f1)\n","    print(\"Confusion Matrix:\", cm)\n","    print()\n","\n","with torch.no_grad():\n","    X_test_tensor, y_test_tensor = X_test_tensor.to(device), y_test_tensor.to(device)\n","    outputs = model(X_test_tensor)\n","    _, predicted = torch.max(outputs, 1)\n","    y_true = torch.argmax(y_test_tensor, dim=1).cpu().numpy()\n","    y_pred = predicted.cpu().numpy()\n","    accuracy = accuracy_score(y_true, y_pred)\n","    precision = precision_score(y_true, y_pred, average='weighted')\n","    recall = recall_score(y_true, y_pred, average='weighted')\n","    f1 = f1_score(y_true, y_pred, average='weighted')\n","    cm = confusion_matrix(y_true, y_pred)\n","    print(\"Test Set:\")\n","    print(\"Accuracy:\", accuracy)\n","    print(\"Precision:\", precision)\n","    print(\"Recall:\", recall)\n","    print(\"F1 Score:\", f1)\n","    print(\"Confusion Matrix:\", cm)\n"]},{"cell_type":"code","execution_count":14,"metadata":{"execution":{"iopub.execute_input":"2024-04-24T09:05:26.249813Z","iopub.status.busy":"2024-04-24T09:05:26.248738Z","iopub.status.idle":"2024-04-24T09:05:26.260569Z","shell.execute_reply":"2024-04-24T09:05:26.259677Z","shell.execute_reply.started":"2024-04-24T09:05:26.249774Z"},"trusted":true},"outputs":[],"source":["class ELMo_Embeddings(nn.Module):\n","    def __init__(self, vocab_size, embedding_dim, hidden_dim, embedding_matrix):\n","        super(ELMo_Embeddings, self).__init__()\n","        self.vocab_size = vocab_size\n","        self.embedding_dim = embedding_dim\n","        self.hidden_dim = hidden_dim\n","        self.embedding1 = nn.Embedding.from_pretrained(embedding_matrix)\n","        self.embedding2 = nn.Embedding.from_pretrained(embedding_matrix)\n","        self.lstm_forward1 = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)\n","        self.lstm_forward2 = nn.LSTM(hidden_dim, hidden_dim, batch_first=True)\n","        self.lstm_backward1 = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)\n","        self.lstm_backward2 = nn.LSTM(hidden_dim, hidden_dim, batch_first=True)\n","        self.linear_mode1 = nn.Linear(200, vocab_size)\n","        self.linear_mode2 = nn.Linear(200, vocab_size)\n","\n","    def forward(self, input_data):\n","        forward_embed = self.embedding1(input_data)\n","        forward_lstm1, _ = self.lstm_forward1(forward_embed) \n","        forward_lstm2, _ = self.lstm_forward2(forward_lstm1)\n","\n","        input_data = torch.flip(input_data, dims=[1])\n","        backward_embed = self.embedding2(input_data)\n","        backward_lstm1, _ = self.lstm_backward1(backward_embed)\n","        backward_lstm2, _ = self.lstm_backward2(backward_lstm1)\n","        backward_lstm1 = torch.flip(backward_lstm1, dims=[1])\n","        backward_lstm2 = torch.flip(backward_lstm2, dims=[1])\n","\n","        e1 = torch.cat((forward_embed, forward_embed), dim=-1)\n","        e2 = torch.cat((forward_lstm1, backward_lstm1), dim=-1)\n","        e3 = torch.cat((forward_lstm2, backward_lstm2), dim=-1)\n","\n","        return e1,e2,e3"]},{"cell_type":"code","execution_count":15,"metadata":{"execution":{"iopub.execute_input":"2024-04-24T09:05:30.372429Z","iopub.status.busy":"2024-04-24T09:05:30.371518Z","iopub.status.idle":"2024-04-24T09:05:30.377992Z","shell.execute_reply":"2024-04-24T09:05:30.377144Z","shell.execute_reply.started":"2024-04-24T09:05:30.372390Z"},"trusted":true},"outputs":[{"data":{"text/plain":["ELMo(\n","  (embedding1): Embedding(32009, 100)\n","  (embedding2): Embedding(32009, 100)\n","  (lstm_forward1): LSTM(100, 100, batch_first=True)\n","  (lstm_forward2): LSTM(100, 100, batch_first=True)\n","  (lstm_backward1): LSTM(100, 100, batch_first=True)\n","  (lstm_backward2): LSTM(100, 100, batch_first=True)\n","  (linear_mode1): Linear(in_features=200, out_features=32009, bias=True)\n","  (linear_mode2): Linear(in_features=200, out_features=32009, bias=True)\n",")"]},"execution_count":15,"metadata":{},"output_type":"execute_result"}],"source":["elmo"]},{"cell_type":"code","execution_count":16,"metadata":{"execution":{"iopub.execute_input":"2024-04-24T09:05:33.299819Z","iopub.status.busy":"2024-04-24T09:05:33.299466Z","iopub.status.idle":"2024-04-24T09:05:33.589374Z","shell.execute_reply":"2024-04-24T09:05:33.588496Z","shell.execute_reply.started":"2024-04-24T09:05:33.299793Z"},"trusted":true},"outputs":[{"data":{"text/plain":["ELMo_Embeddings(\n","  (embedding1): Embedding(32009, 100)\n","  (embedding2): Embedding(32009, 100)\n","  (lstm_forward1): LSTM(100, 100, batch_first=True)\n","  (lstm_forward2): LSTM(100, 100, batch_first=True)\n","  (lstm_backward1): LSTM(100, 100, batch_first=True)\n","  (lstm_backward2): LSTM(100, 100, batch_first=True)\n","  (linear_mode1): Linear(in_features=200, out_features=32009, bias=True)\n","  (linear_mode2): Linear(in_features=200, out_features=32009, bias=True)\n",")"]},"execution_count":16,"metadata":{},"output_type":"execute_result"}],"source":["elmo_embed=ELMo_Embeddings(vocab_size, embedding_dim, hidden_dim, embedding_matrix)\n","elmo_embed.load_state_dict(elmo.state_dict())\n","elmo_embed.to(device)"]},{"cell_type":"code","execution_count":17,"metadata":{"execution":{"iopub.execute_input":"2024-04-24T09:05:37.624544Z","iopub.status.busy":"2024-04-24T09:05:37.624201Z","iopub.status.idle":"2024-04-24T09:05:41.503073Z","shell.execute_reply":"2024-04-24T09:05:41.502246Z","shell.execute_reply.started":"2024-04-24T09:05:37.624517Z"},"trusted":true},"outputs":[],"source":["X_train = []\n","for sentence in sentences_train:\n","    if len(sentence) < length_sentence:\n","        padding_needed = length_sentence - len(sentence)\n","        sentence.extend(padding_needed*[word2id[PAD_TOKEN]])\n","    X_train.append(torch.tensor(sentence[:length_sentence]))\n","y_train = pd.get_dummies(train_data['Class Index'], prefix='value', dtype=int).values\n","\n","X_test = []\n","for sentence in sentences_test:\n","    if len(sentence) < length_sentence:\n","        padding_needed = length_sentence - len(sentence)\n","        sentence.extend(padding_needed*[word2id[PAD_TOKEN]])\n","    X_test.append(torch.tensor(sentence[:length_sentence]))\n","y_test = pd.get_dummies(test_data['Class Index'], prefix='value', dtype=int).values\n","\n","\n","X_train_tensor = torch.stack(X_train)\n","X_test_tensor = torch.stack(X_test)\n","y_train_tensor = torch.tensor(y_train,dtype=torch.float32)\n","y_test_tensor = torch.tensor(y_test,dtype=torch.float32)\n","\n","train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n","train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n","test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n","test_loader = DataLoader(test_dataset, batch_size=32, shuffle=True)"]},{"cell_type":"code","execution_count":18,"metadata":{"execution":{"iopub.execute_input":"2024-04-24T09:05:43.742034Z","iopub.status.busy":"2024-04-24T09:05:43.741329Z","iopub.status.idle":"2024-04-24T09:05:43.749357Z","shell.execute_reply":"2024-04-24T09:05:43.748351Z","shell.execute_reply.started":"2024-04-24T09:05:43.742004Z"},"trusted":true},"outputs":[],"source":["class LSTMModel_Trainable(nn.Module):\n","    def __init__(self, input_size, hidden_size, output_size):\n","        super(LSTMModel_Trainable, self).__init__()\n","        self.weights=nn.Parameter(torch.tensor([0.33,0.33,0.33]))\n","        self.lstm = nn.LSTM(input_size, hidden_size, batch_first=True)\n","        self.fc = nn.Linear(hidden_size, output_size)\n","\n","    def forward(self, e1, e2, e3):\n","        weights_softmax = torch.nn.functional.softmax(self.weights, dim=0)\n","        x = e1 * weights_softmax[0] + e2 * weights_softmax[1] + e3 * weights_softmax[2]\n","        out, _ = self.lstm(x)\n","        out = self.fc(out[:, -1, :])\n","        return out"]},{"cell_type":"code","execution_count":19,"metadata":{"execution":{"iopub.execute_input":"2024-04-24T09:05:46.760796Z","iopub.status.busy":"2024-04-24T09:05:46.760442Z","iopub.status.idle":"2024-04-24T09:10:12.304124Z","shell.execute_reply":"2024-04-24T09:10:12.303172Z","shell.execute_reply.started":"2024-04-24T09:05:46.760767Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["100%|██████████| 3750/3750 [00:26<00:00, 142.77it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 1/10, Loss: 0.4739084247479836\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 3750/3750 [00:25<00:00, 149.79it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 2/10, Loss: 0.29628586613237856\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 3750/3750 [00:25<00:00, 149.62it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 3/10, Loss: 0.257423589746654\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 3750/3750 [00:24<00:00, 150.09it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 4/10, Loss: 0.23004856839577356\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 3750/3750 [00:24<00:00, 150.28it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 5/10, Loss: 0.2076663779253761\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 3750/3750 [00:25<00:00, 149.44it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 6/10, Loss: 0.18900374225589137\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 3750/3750 [00:25<00:00, 149.85it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 7/10, Loss: 0.16953803968106707\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 3750/3750 [00:25<00:00, 149.41it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 8/10, Loss: 0.1510261582493782\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 3750/3750 [00:24<00:00, 150.14it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 9/10, Loss: 0.13423869293220342\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 3750/3750 [00:25<00:00, 149.95it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 10/10, Loss: 0.1179233610022813\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 3750/3750 [00:10<00:00, 368.64it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Train Set:\n","Accuracy: 0.968725\n","Precision: 0.9688810255311249\n","Recall: 0.968725\n","F1 Score: 0.9687664397217453\n","Confusion Matrix: [[28908   157   595   340]\n"," [   89 29830    40    41]\n"," [  128    45 28717  1110]\n"," [  261    20   927 28792]]\n","\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 238/238 [00:00<00:00, 365.61it/s]"]},{"name":"stdout","output_type":"stream","text":["Test Set:\n","Accuracy: 0.9182894736842105\n","Precision: 0.9185391091489916\n","Recall: 0.9182894736842105\n","F1 Score: 0.9183672611504463\n","Confusion Matrix: [[1736   32   83   49]\n"," [  14 1852   20   14]\n"," [  46    8 1694  152]\n"," [  59   14  130 1697]]\n","\n"]},{"name":"stderr","output_type":"stream","text":["\n"]}],"source":["input_size = 200  \n","hidden_size = 128\n","output_size = 4\n","model = LSTMModel_Trainable(input_size, hidden_size, output_size).to(device)\n","\n","criterion = nn.CrossEntropyLoss(ignore_index=word2id[PAD_TOKEN])\n","optimizer = optim.Adam(model.parameters(), lr=0.001)\n","\n","train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n","train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n","\n","epochs = 10\n","for epoch in range(epochs):\n","    model.train()\n","    total_loss = 0.0\n","    total_samples = 0\n","    \n","    for batch_X, batch_y in tqdm(train_loader):\n","        batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n","        e1, e2, e3 = elmo_embed(batch_X)\n","        e1, e2, e3 = e1.to(device), e2.to(device), e3.to(device)\n","        optimizer.zero_grad()\n","        outputs = model(e1, e2, e3)\n","        _, target_indices = batch_y.max(dim=1)\n","        loss = criterion(outputs, target_indices)\n","        loss.backward()\n","        optimizer.step()\n","        total_loss += loss.item() * batch_X.size(0)\n","        total_samples += batch_X.size(0)\n","    epoch_loss = total_loss / total_samples\n","    print(f\"Epoch {epoch+1}/{epochs}, Loss: {epoch_loss}\")\n","\n","model.eval()\n","with torch.no_grad():\n","    y_true = []\n","    y_pred = []\n","    for batch_X, batch_y in tqdm(train_loader):\n","        batch_X = batch_X.to(device)\n","        e1, e2, e3 = elmo_embed(batch_X)\n","        e1, e2, e3 = e1.to(device), e2.to(device), e3.to(device)\n","        outputs = model(e1, e2, e3)\n","        _, predicted = torch.max(outputs, 1)\n","        y_true.extend(torch.argmax(batch_y, dim=1).cpu().numpy())  \n","        y_pred.extend(predicted.cpu().numpy())\n","    accuracy = accuracy_score(y_true, y_pred)\n","    precision = precision_score(y_true, y_pred, average='weighted')\n","    recall = recall_score(y_true, y_pred, average='weighted')\n","    f1 = f1_score(y_true, y_pred, average='weighted')\n","    cm = confusion_matrix(y_true, y_pred)\n","    print(\"Train Set:\")\n","    print(\"Accuracy:\", accuracy)\n","    print(\"Precision:\", precision)\n","    print(\"Recall:\", recall)\n","    print(\"F1 Score:\", f1)\n","    print(\"Confusion Matrix:\", cm)\n","    print()\n","    \n","model.eval()\n","with torch.no_grad():\n","    y_true = []\n","    y_pred = []\n","    for batch_X, batch_y in tqdm(test_loader):\n","        batch_X = batch_X.to(device)\n","        e1, e2, e3 = elmo_embed(batch_X)\n","        e1, e2, e3 = e1.to(device), e2.to(device), e3.to(device)\n","        outputs = model(e1, e2, e3)\n","        _, predicted = torch.max(outputs, 1)\n","        y_true.extend(torch.argmax(batch_y, dim=1).cpu().numpy())  \n","        y_pred.extend(predicted.cpu().numpy())\n","    accuracy = accuracy_score(y_true, y_pred)\n","    precision = precision_score(y_true, y_pred, average='weighted')\n","    recall = recall_score(y_true, y_pred, average='weighted')\n","    f1 = f1_score(y_true, y_pred, average='weighted')\n","    cm = confusion_matrix(y_true, y_pred)\n","    print(\"Test Set:\")\n","    print(\"Accuracy:\", accuracy)\n","    print(\"Precision:\", precision)\n","    print(\"Recall:\", recall)\n","    print(\"F1 Score:\", f1)\n","    print(\"Confusion Matrix:\", cm)\n","    print()"]},{"cell_type":"code","execution_count":32,"metadata":{"execution":{"iopub.execute_input":"2024-04-24T10:01:13.443311Z","iopub.status.busy":"2024-04-24T10:01:13.442787Z","iopub.status.idle":"2024-04-24T10:01:13.451355Z","shell.execute_reply":"2024-04-24T10:01:13.450465Z","shell.execute_reply.started":"2024-04-24T10:01:13.443269Z"},"trusted":true},"outputs":[],"source":["class LSTMModel_Frozen(nn.Module):\n","    def __init__(self, input_size, hidden_size, output_size):\n","        super(LSTMModel_Frozen, self).__init__()\n","        self.weights = nn.Parameter(torch.randn(3), requires_grad=False)\n","        self.lstm = nn.LSTM(input_size, hidden_size, batch_first=True)\n","        self.fc = nn.Linear(hidden_size, output_size)\n","\n","    def forward(self, e1, e2, e3):\n","        weights_softmax = torch.nn.functional.softmax(self.weights, dim=0)\n","        x = e1 * weights_softmax[0] + e2 * weights_softmax[1] + e3 * weights_softmax[2]\n","        out, _ = self.lstm(x)\n","        out = self.fc(out[:, -1, :])\n","        return out"]},{"cell_type":"code","execution_count":33,"metadata":{"execution":{"iopub.execute_input":"2024-04-24T10:01:17.701126Z","iopub.status.busy":"2024-04-24T10:01:17.700806Z","iopub.status.idle":"2024-04-24T10:05:35.246381Z","shell.execute_reply":"2024-04-24T10:05:35.245495Z","shell.execute_reply.started":"2024-04-24T10:01:17.701101Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["100%|██████████| 3750/3750 [00:24<00:00, 152.84it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 1/10, Loss: 0.48557809265752633\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 3750/3750 [00:24<00:00, 152.66it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 2/10, Loss: 0.3207686462908983\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 3750/3750 [00:24<00:00, 151.88it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 3/10, Loss: 0.28268957933982214\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 3750/3750 [00:24<00:00, 153.10it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 4/10, Loss: 0.2565281331380208\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 3750/3750 [00:24<00:00, 151.74it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 5/10, Loss: 0.23706546026170253\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 3750/3750 [00:24<00:00, 152.23it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 6/10, Loss: 0.21902322891006867\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 3750/3750 [00:24<00:00, 152.38it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 7/10, Loss: 0.20462748363912106\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 3750/3750 [00:24<00:00, 152.07it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 8/10, Loss: 0.190780981417497\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 3750/3750 [00:24<00:00, 152.34it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 9/10, Loss: 0.17691761648207902\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 3750/3750 [00:24<00:00, 152.05it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 10/10, Loss: 0.16341493985429406\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 3750/3750 [00:10<00:00, 369.66it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Train Set:\n","Accuracy: 0.947025\n","Precision: 0.9471167941478705\n","Recall: 0.947025\n","F1 Score: 0.9470310359340889\n","Confusion Matrix: [[28362   318   827   493]\n"," [   66 29771   121    42]\n"," [  434   108 27690  1768]\n"," [  483   122  1575 27820]]\n","\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 238/238 [00:00<00:00, 372.70it/s]"]},{"name":"stdout","output_type":"stream","text":["Test Set:\n","Accuracy: 0.9140789473684211\n","Precision: 0.9142387814401838\n","Recall: 0.9140789473684211\n","F1 Score: 0.9140920125807337\n","Confusion Matrix: [[1739   38   73   50]\n"," [  12 1853   27    8]\n"," [  43   17 1677  163]\n"," [  54   19  149 1678]]\n","\n"]},{"name":"stderr","output_type":"stream","text":["\n"]}],"source":["input_size = 200  \n","hidden_size = 128\n","output_size = 4\n","model = LSTMModel_Frozen(input_size, hidden_size, output_size).to(device)\n","\n","criterion = nn.CrossEntropyLoss(ignore_index=word2id[PAD_TOKEN])\n","optimizer = optim.Adam(model.parameters(), lr=0.001)\n","\n","train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n","train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n","\n","epochs = 10\n","for epoch in range(epochs):\n","    model.train()\n","    total_loss = 0.0\n","    total_samples = 0\n","    \n","    for batch_X, batch_y in tqdm(train_loader):\n","        batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n","        e1, e2, e3 = elmo_embed(batch_X)\n","        e1, e2, e3 = e1.to(device), e2.to(device), e3.to(device)\n","        optimizer.zero_grad()\n","        outputs = model(e1, e2, e3)\n","        _, target_indices = batch_y.max(dim=1)\n","        loss = criterion(outputs, target_indices)\n","        loss.backward()\n","        optimizer.step()\n","        total_loss += loss.item() * batch_X.size(0)\n","        total_samples += batch_X.size(0)\n","    epoch_loss = total_loss / total_samples\n","    print(f\"Epoch {epoch+1}/{epochs}, Loss: {epoch_loss}\")\n","\n","model.eval()\n","with torch.no_grad():\n","    y_true = []\n","    y_pred = []\n","    for batch_X, batch_y in tqdm(train_loader):\n","        batch_X = batch_X.to(device)\n","        e1, e2, e3 = elmo_embed(batch_X)\n","        e1, e2, e3 = e1.to(device), e2.to(device), e3.to(device)\n","        outputs = model(e1, e2, e3)\n","        _, predicted = torch.max(outputs, 1)\n","        y_true.extend(torch.argmax(batch_y, dim=1).cpu().numpy())  \n","        y_pred.extend(predicted.cpu().numpy())\n","    accuracy = accuracy_score(y_true, y_pred)\n","    precision = precision_score(y_true, y_pred, average='weighted')\n","    recall = recall_score(y_true, y_pred, average='weighted')\n","    f1 = f1_score(y_true, y_pred, average='weighted')\n","    cm = confusion_matrix(y_true, y_pred)\n","    print(\"Train Set:\")\n","    print(\"Accuracy:\", accuracy)\n","    print(\"Precision:\", precision)\n","    print(\"Recall:\", recall)\n","    print(\"F1 Score:\", f1)\n","    print(\"Confusion Matrix:\", cm)\n","    print()\n","    \n","model.eval()\n","with torch.no_grad():\n","    y_true = []\n","    y_pred = []\n","    for batch_X, batch_y in tqdm(test_loader):\n","        batch_X = batch_X.to(device)\n","        e1, e2, e3 = elmo_embed(batch_X)\n","        e1, e2, e3 = e1.to(device), e2.to(device), e3.to(device)\n","        outputs = model(e1, e2, e3)\n","        _, predicted = torch.max(outputs, 1)\n","        y_true.extend(torch.argmax(batch_y, dim=1).cpu().numpy())  \n","        y_pred.extend(predicted.cpu().numpy())\n","    accuracy = accuracy_score(y_true, y_pred)\n","    precision = precision_score(y_true, y_pred, average='weighted')\n","    recall = recall_score(y_true, y_pred, average='weighted')\n","    f1 = f1_score(y_true, y_pred, average='weighted')\n","    cm = confusion_matrix(y_true, y_pred)\n","    print(\"Test Set:\")\n","    print(\"Accuracy:\", accuracy)\n","    print(\"Precision:\", precision)\n","    print(\"Recall:\", recall)\n","    print(\"F1 Score:\", f1)\n","    print(\"Confusion Matrix:\", cm)\n","    print()"]},{"cell_type":"code","execution_count":36,"metadata":{"execution":{"iopub.execute_input":"2024-04-24T10:30:06.006392Z","iopub.status.busy":"2024-04-24T10:30:06.006017Z","iopub.status.idle":"2024-04-24T10:30:06.013647Z","shell.execute_reply":"2024-04-24T10:30:06.012645Z","shell.execute_reply.started":"2024-04-24T10:30:06.006363Z"},"trusted":true},"outputs":[],"source":["class LSTMModel_LearnableFunction(nn.Module):\n","    def __init__(self, input_size, hidden_size, output_size):\n","        super(LSTMModel_LearnableFunction, self).__init__()\n","        self.lstm = nn.LSTM(input_size, hidden_size, batch_first=True)\n","        self.fc1 = nn.Linear(hidden_size, 512)  \n","        self.fc2 = nn.Linear(512, output_size)\n","\n","    def forward(self, e1, e2, e3):\n","        concatenated_embedding = torch.cat((e1, e2, e3), dim=1)\n","        lstm_output, _ = self.lstm(concatenated_embedding)\n","        lstm_output_last = lstm_output[:, -1, :]\n","        output = torch.relu(self.fc1(lstm_output_last))\n","        output = self.fc2(output)\n","        return output\n"]},{"cell_type":"code","execution_count":37,"metadata":{"execution":{"iopub.execute_input":"2024-04-24T10:30:09.288525Z","iopub.status.busy":"2024-04-24T10:30:09.287835Z","iopub.status.idle":"2024-04-24T10:35:29.792498Z","shell.execute_reply":"2024-04-24T10:35:29.791584Z","shell.execute_reply.started":"2024-04-24T10:30:09.288491Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["100%|██████████| 3750/3750 [00:31<00:00, 118.91it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 1/10, Loss: 0.425501187240084\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 3750/3750 [00:30<00:00, 122.51it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 2/10, Loss: 0.3329407417173187\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 3750/3750 [00:30<00:00, 122.58it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 3/10, Loss: 0.3003496779024601\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 3750/3750 [00:30<00:00, 122.28it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 4/10, Loss: 0.27615042241513726\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 3750/3750 [00:30<00:00, 122.72it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 5/10, Loss: 0.2585706581488252\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 3750/3750 [00:30<00:00, 122.91it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 6/10, Loss: 0.24218030960609516\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 3750/3750 [00:30<00:00, 122.87it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 7/10, Loss: 0.22789891442507507\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 3750/3750 [00:30<00:00, 122.78it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 8/10, Loss: 0.21410267429426313\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 3750/3750 [00:30<00:00, 122.71it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 9/10, Loss: 0.1998341369693478\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 3750/3750 [00:30<00:00, 122.72it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 10/10, Loss: 0.18788738073756298\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 3750/3750 [00:12<00:00, 300.02it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Train Set:\n","Accuracy: 0.939475\n","Precision: 0.940556274108918\n","Recall: 0.939475\n","F1 Score: 0.9395294676040294\n","Confusion Matrix: [[27887   344   724  1045]\n"," [  165 29585    87   163]\n"," [  549   196 26771  2484]\n"," [  285   115  1106 28494]]\n","\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 238/238 [00:00<00:00, 300.17it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Test Set:\n","Accuracy: 0.9017105263157895\n","Precision: 0.9027609059432867\n","Recall: 0.9017105263157895\n","F1 Score: 0.9016840605747216\n","Confusion Matrix: [[1695   43   73   89]\n"," [  25 1835   15   25]\n"," [  60   31 1600  209]\n"," [  46   17  114 1723]]\n","\n"]}],"source":["input_size = 200  \n","hidden_size = 128\n","output_size = 4\n","model = LSTMModel_LearnableFunction(input_size, hidden_size, output_size).to(device)\n","\n","criterion = nn.CrossEntropyLoss(ignore_index=word2id[PAD_TOKEN])\n","optimizer = optim.Adam(model.parameters(), lr=0.001)\n","\n","train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n","train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n","\n","epochs = 10\n","for epoch in range(epochs):\n","    model.train()\n","    total_loss = 0.0\n","    total_samples = 0\n","    \n","    for batch_X, batch_y in tqdm(train_loader):\n","        batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n","        e1, e2, e3 = elmo_embed(batch_X)\n","        e1, e2, e3 = e1.to(device), e2.to(device), e3.to(device)\n","        optimizer.zero_grad()\n","        outputs = model(e1, e2, e3)\n","        _, target_indices = batch_y.max(dim=1)\n","        loss = criterion(outputs, target_indices)\n","        loss.backward()\n","        optimizer.step()\n","        total_loss += loss.item() * batch_X.size(0)\n","        total_samples += batch_X.size(0)\n","    epoch_loss = total_loss / total_samples\n","    print(f\"Epoch {epoch+1}/{epochs}, Loss: {epoch_loss}\")\n","\n","model.eval()\n","with torch.no_grad():\n","    y_true = []\n","    y_pred = []\n","    for batch_X, batch_y in tqdm(train_loader):\n","        batch_X = batch_X.to(device)\n","        e1, e2, e3 = elmo_embed(batch_X)\n","        e1, e2, e3 = e1.to(device), e2.to(device), e3.to(device)\n","        outputs = model(e1, e2, e3)\n","        _, predicted = torch.max(outputs, 1)\n","        y_true.extend(torch.argmax(batch_y, dim=1).cpu().numpy())  \n","        y_pred.extend(predicted.cpu().numpy())\n","    accuracy = accuracy_score(y_true, y_pred)\n","    precision = precision_score(y_true, y_pred, average='weighted')\n","    recall = recall_score(y_true, y_pred, average='weighted')\n","    f1 = f1_score(y_true, y_pred, average='weighted')\n","    cm = confusion_matrix(y_true, y_pred)\n","    print(\"Train Set:\")\n","    print(\"Accuracy:\", accuracy)\n","    print(\"Precision:\", precision)\n","    print(\"Recall:\", recall)\n","    print(\"F1 Score:\", f1)\n","    print(\"Confusion Matrix:\", cm)\n","    print()\n","    \n","model.eval()\n","with torch.no_grad():\n","    y_true = []\n","    y_pred = []\n","    for batch_X, batch_y in tqdm(test_loader):\n","        batch_X = batch_X.to(device)\n","        e1, e2, e3 = elmo_embed(batch_X)\n","        e1, e2, e3 = e1.to(device), e2.to(device), e3.to(device)\n","        outputs = model(e1, e2, e3)\n","        _, predicted = torch.max(outputs, 1)\n","        y_true.extend(torch.argmax(batch_y, dim=1).cpu().numpy())  \n","        y_pred.extend(predicted.cpu().numpy())\n","    accuracy = accuracy_score(y_true, y_pred)\n","    precision = precision_score(y_true, y_pred, average='weighted')\n","    recall = recall_score(y_true, y_pred, average='weighted')\n","    f1 = f1_score(y_true, y_pred, average='weighted')\n","    cm = confusion_matrix(y_true, y_pred)\n","    print(\"Test Set:\")\n","    print(\"Accuracy:\", accuracy)\n","    print(\"Precision:\", precision)\n","    print(\"Recall:\", recall)\n","    print(\"F1 Score:\", f1)\n","    print(\"Confusion Matrix:\", cm)\n","    print()"]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"datasetId":4628994,"sourceId":7885595,"sourceType":"datasetVersion"},{"datasetId":4860670,"sourceId":8204049,"sourceType":"datasetVersion"},{"datasetId":4864949,"sourceId":8209560,"sourceType":"datasetVersion"},{"datasetId":4867617,"sourceId":8213079,"sourceType":"datasetVersion"},{"isSourceIdPinned":true,"modelInstanceId":31234,"sourceId":37103,"sourceType":"modelInstanceVersion"}],"dockerImageVersionId":30699,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":4}
