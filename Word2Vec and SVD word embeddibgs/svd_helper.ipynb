{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "from collections import defaultdict\n",
    "from scipy.sparse import csr_matrix\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "\n",
    "UNKNOWN_TOKEN='<UNK>'\n",
    "PAD_TOKEN='<PAD>'\n",
    "START_TOKEN='<START>'\n",
    "END_TOKEN='<END>'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data=pd.read_csv('./train.csv')\n",
    "test_data=pd.read_csv('./test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(data,type='train'):\n",
    "    sentences=[]\n",
    "    vocab=set()\n",
    "    vocab.add(PAD_TOKEN)\n",
    "    vocab.add(UNKNOWN_TOKEN)\n",
    "\n",
    "    frequency=dict()\n",
    "    for text in data:\n",
    "        text = re.sub(r'[^\\w\\s\\n]', ' ', str(text).lower())\n",
    "        words = word_tokenize(text)\n",
    "        words=[START_TOKEN]+words+[END_TOKEN]\n",
    "        sentences.append(words)\n",
    "        for word in words:\n",
    "            frequency[word]=frequency.get(word,0)+1\n",
    "    \n",
    "    if type=='train':\n",
    "        frequency_threshold=3\n",
    "        for i in range(len(sentences)):\n",
    "            for j in range(len(sentences[i])):\n",
    "                if frequency[sentences[i][j]]<frequency_threshold:\n",
    "                    sentences[i][j]=UNKNOWN_TOKEN\n",
    "\n",
    "    for sentence in sentences:\n",
    "        for word in sentence:\n",
    "            vocab.add(word)\n",
    "\n",
    "    return sentences,vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_co_occurrence_matrix(tokens_list, window_size=5):\n",
    "    co_occurrence_matrix = defaultdict(int)\n",
    "    for tokens in tokens_list:\n",
    "        for i in range(len(tokens)):\n",
    "            for j in range(max(0, i - window_size), min(len(tokens), i + window_size + 1)):\n",
    "                if i != j:\n",
    "                    co_occurrence_matrix[(tokens[i], tokens[j])] += 1\n",
    "    return co_occurrence_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_svd(co_matrix,vocab,vector_size=300):\n",
    "    words = vocab\n",
    "    word_index = {word: i for i, word in enumerate(words)}    \n",
    "    rows, cols, data = [], [], []\n",
    "    for (word1, word2), count in co_matrix.items():\n",
    "        rows.append(word_index[word1])\n",
    "        cols.append(word_index[word2])\n",
    "        data.append(count)\n",
    "    co_occurrence_matrix = csr_matrix((data, (rows, cols)), shape=(len(words), len(words)))\n",
    "    svd = TruncatedSVD(n_components=vector_size)\n",
    "    word_vectors_svd = svd.fit_transform(co_occurrence_matrix)\n",
    "    return word_vectors_svd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_train,vocab = preprocess_text(train_data['Description'])\n",
    "sentences_test,_ = preprocess_text(test_data['Description'],'test')\n",
    "co_occurrence_matrix = build_co_occurrence_matrix(sentences_train)\n",
    "word_vectors_svd = apply_svd(co_occurrence_matrix,vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_lengths = [len(sentence) for sentence in sentences_train]\n",
    "sorted_lengths = sorted(sentence_lengths)\n",
    "index_95th_percentile = int(np.percentile(range(len(sorted_lengths)), 95))\n",
    "length_95th_percentile = sorted_lengths[index_95th_percentile]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_index = {word: i for i, word in enumerate(vocab)}    \n",
    "\n",
    "length_sentence=length_95th_percentile\n",
    "\n",
    "X_train = []\n",
    "for sentence in sentences_train:\n",
    "    sentence_embedding = [word_vectors_svd[word_index[word]] for word in sentence]\n",
    "    if len(sentence)<length_sentence:\n",
    "        padding_needed = length_sentence - len(sentence)\n",
    "        sentence_embedding.extend(padding_needed*[word_vectors_svd[word_index[PAD_TOKEN]]])\n",
    "    if sentence_embedding:\n",
    "        X_train.append((sentence_embedding[:length_sentence]))\n",
    "y_train = pd.get_dummies(train_data['Class Index'], prefix='value', dtype=int).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = []\n",
    "for sentence in sentences_test:\n",
    "    sentence_embedding = [word_vectors_svd[word_index.get(word,word_index[UNKNOWN_TOKEN])] for word in sentence]\n",
    "    if len(sentence)<length_sentence:\n",
    "        padding_needed = length_sentence - len(sentence)\n",
    "        sentence_embedding.extend(padding_needed*[word_vectors_svd[word_index[PAD_TOKEN]]])\n",
    "    if sentence_embedding:\n",
    "        X_test.append((sentence_embedding[:length_sentence]))\n",
    "y_test = pd.get_dummies(test_data['Class Index'], prefix='value', dtype=int).values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNClassifier(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(RNNClassifier, self).__init__()\n",
    "        self.rnn = nn.LSTM(input_size, hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        output, _ = self.rnn(x)\n",
    "        output = self.fc(output[:, -1, :]) \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1275404/3299132171.py:9: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:275.)\n",
      "  X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n"
     ]
    }
   ],
   "source": [
    "input_size = 300  \n",
    "hidden_size = 128\n",
    "output_size = 4\n",
    "model = RNNClassifier(input_size, hidden_size, output_size)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 0.37698838114738464\n",
      "Epoch 2/10, Loss: 0.335050106048584\n",
      "Epoch 3/10, Loss: 0.3184419870376587\n",
      "Epoch 4/10, Loss: 0.22597351670265198\n",
      "Epoch 5/10, Loss: 0.43549844622612\n",
      "Epoch 6/10, Loss: 0.3450637757778168\n",
      "Epoch 7/10, Loss: 0.34369587898254395\n",
      "Epoch 8/10, Loss: 0.09720759838819504\n",
      "Epoch 9/10, Loss: 0.4278640151023865\n",
      "Epoch 10/10, Loss: 0.11683081090450287\n"
     ]
    }
   ],
   "source": [
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "epochs = 10\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    for batch_X, batch_y in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(batch_X)\n",
    "        loss = criterion(outputs, batch_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(f\"Epoch {epoch+1}/{epochs}, Loss: {loss.item()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([120000, 53, 300])\n",
      "torch.Size([120000, 4])\n",
      "torch.Size([7600, 53, 300])\n",
      "torch.Size([7600, 4])\n"
     ]
    }
   ],
   "source": [
    "print(X_train_tensor.size())\n",
    "print(y_train_tensor.size())\n",
    "print(X_test_tensor.size())\n",
    "print(y_test_tensor.size())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8888157894736842\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    outputs = model(X_test_tensor)\n",
    "    _, predicted = torch.max(outputs, 1)\n",
    "    accuracy = accuracy_score(torch.argmax(y_test_tensor, dim=1).numpy(), predicted.numpy())\n",
    "    print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from collections import defaultdict\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "PAD_TOKEN = '<PAD>'\n",
    "UNKNOWN_TOKEN = '<UNKNOWN>'\n",
    "START_TOKEN = '<START>'\n",
    "END_TOKEN = '<END>'\n",
    "\n",
    "class SVDWordEmbeddings:\n",
    "    def __init__(self, data, frequency_threshold=3, window_size=5, vector_size=300):\n",
    "        self.data = data\n",
    "        self.frequency_threshold = frequency_threshold\n",
    "        self.window_size = window_size\n",
    "        self.vector_size = vector_size\n",
    "        self.vocab = set()\n",
    "        self.vocab.add(PAD_TOKEN)\n",
    "        self.vocab.add(UNKNOWN_TOKEN)\n",
    "        self.sentences = []\n",
    "        self.frequency = defaultdict(int)\n",
    "        self.co_occurrence_matrix = defaultdict(int)\n",
    "        self.word_vectors = None\n",
    "\n",
    "    def preprocess_text(self, type='train'):\n",
    "        for text in self.data:\n",
    "            text = re.sub(r'[^\\w\\s\\n]', ' ', str(text).lower())\n",
    "            words = word_tokenize(text)\n",
    "            words = [START_TOKEN] + words + [END_TOKEN]\n",
    "            self.sentences.append(words)\n",
    "            for word in words:\n",
    "                self.frequency[word] += 1\n",
    "\n",
    "        if(type == 'train'):\n",
    "            for i in range(len(self.sentences)):\n",
    "                for j in range(len(self.sentences[i])):\n",
    "                    if self.frequency[self.sentences[i][j]] < self.frequency_threshold:\n",
    "                        self.sentences[i][j] = UNKNOWN_TOKEN\n",
    "\n",
    "        for sentence in self.sentences:\n",
    "            for word in sentence:\n",
    "                self.vocab.add(word)\n",
    "\n",
    "    def build_co_occurrence_matrix(self):\n",
    "        for tokens in self.sentences:\n",
    "            for i in range(len(tokens)):\n",
    "                for j in range(max(0, i - self.window_size), min(len(tokens), i + self.window_size + 1)):\n",
    "                    if i != j:\n",
    "                        self.co_occurrence_matrix[(tokens[i], tokens[j])] += 1\n",
    "\n",
    "    def apply_svd(self):\n",
    "        words = list(self.vocab)\n",
    "        self.word_index = {word: i for i, word in enumerate(words)}\n",
    "        rows, cols, data = [], [], []\n",
    "        for (word1, word2), count in self.co_occurrence_matrix.items():\n",
    "            rows.append(self.word_index[word1])\n",
    "            cols.append(self.word_index[word2])\n",
    "            data.append(count)\n",
    "        co_occurrence_matrix = csr_matrix((data, (rows, cols)), shape=(len(words), len(words)))\n",
    "        svd = TruncatedSVD(n_components=self.vector_size)\n",
    "        self.word_vectors = svd.fit_transform(co_occurrence_matrix)\n",
    "\n",
    "    def fit(self):\n",
    "        self.preprocess_text()\n",
    "        self.build_co_occurrence_matrix()\n",
    "        self.apply_svd()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_model_train = SVDWordEmbeddings(train_data['Description'])\n",
    "embeddings_model_train.fit()\n",
    "sentences_train=embeddings_model_train.sentences\n",
    "word_index=embeddings_model_train.word_index\n",
    "word_vectors_svd=embeddings_model_train.word_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_model_test = SVDWordEmbeddings(test_data['Description'])\n",
    "embeddings_model_test.preprocess_text('test')\n",
    "sentences_test=embeddings_model_test.sentences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_lengths = [len(sentence) for sentence in sentences_train]\n",
    "sorted_lengths = sorted(sentence_lengths)\n",
    "index_95th_percentile = int(np.percentile(range(len(sorted_lengths)), 95))\n",
    "length_95th_percentile = sorted_lengths[index_95th_percentile]\n",
    "length_sentence=length_95th_percentile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = []\n",
    "for sentence in sentences_train:\n",
    "    sentence_embedding = [word_vectors_svd[word_index[word]] for word in sentence]\n",
    "    if len(sentence)<length_sentence:\n",
    "        padding_needed = length_sentence - len(sentence)\n",
    "        sentence_embedding.extend(padding_needed*[word_vectors_svd[word_index[PAD_TOKEN]]])\n",
    "    if sentence_embedding:\n",
    "        X_train.append((sentence_embedding[:length_sentence]))\n",
    "y_train = pd.get_dummies(train_data['Class Index'], prefix='value', dtype=int).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = []\n",
    "for sentence in sentences_test:\n",
    "    sentence_embedding = [word_vectors_svd[word_index.get(word,word_index[UNKNOWN_TOKEN])] for word in sentence]\n",
    "    if len(sentence)<length_sentence:\n",
    "        padding_needed = length_sentence - len(sentence)\n",
    "        sentence_embedding.extend(padding_needed*[word_vectors_svd[word_index[PAD_TOKEN]]])\n",
    "    if sentence_embedding:\n",
    "        X_test.append((sentence_embedding[:length_sentence]))\n",
    "y_test = pd.get_dummies(test_data['Class Index'], prefix='value', dtype=int).values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNClassifier(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(RNNClassifier, self).__init__()\n",
    "        self.rnn = nn.LSTM(input_size, hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        output, _ = self.rnn(x)\n",
    "        output = self.fc(output[:, -1, :]) \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1313583/3858250530.py:1: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:275.)\n",
      "  X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n"
     ]
    }
   ],
   "source": [
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 0.20513449609279633\n",
      "Epoch 2/10, Loss: 0.3971790075302124\n",
      "Epoch 3/10, Loss: 0.23672860860824585\n",
      "Epoch 4/10, Loss: 0.32392191886901855\n",
      "Epoch 5/10, Loss: 0.6454268097877502\n",
      "Epoch 6/10, Loss: 0.19698625802993774\n",
      "Epoch 7/10, Loss: 0.22427421808242798\n",
      "Epoch 8/10, Loss: 0.4747859835624695\n",
      "Epoch 9/10, Loss: 0.27186471223831177\n",
      "Epoch 10/10, Loss: 0.4595867693424225\n"
     ]
    }
   ],
   "source": [
    "input_size = 300\n",
    "hidden_size = 128\n",
    "output_size = 4\n",
    "model = RNNClassifier(input_size, hidden_size, output_size)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "epochs = 10\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    for batch_X, batch_y in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(batch_X)\n",
    "        loss = criterion(outputs, batch_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(f\"Epoch {epoch+1}/{epochs}, Loss: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Set:\n",
      "Accuracy: 0.9077416666666667\n",
      "Precision: 0.9079916119093905\n",
      "Recall: 0.9077416666666667\n",
      "F1 Score: 0.907734608689379\n",
      "\n",
      "Test Set:\n",
      "Accuracy: 0.8886842105263157\n",
      "Precision: 0.8888566575936085\n",
      "Recall: 0.8886842105263157\n",
      "F1 Score: 0.8886744455785405\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    correct_samples=0\n",
    "    total_samples=0\n",
    "    for batch_X, batch_y in train_loader:\n",
    "        outputs = model(batch_X)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        y_true.extend(torch.argmax(batch_y, dim=1).numpy())\n",
    "        y_pred.extend(predicted.numpy())\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    precision = precision_score(y_true, y_pred, average='weighted')\n",
    "    recall = recall_score(y_true, y_pred, average='weighted')\n",
    "    f1 = f1_score(y_true, y_pred, average='weighted')\n",
    "    print(\"Train Set:\")\n",
    "    print(\"Accuracy:\",accuracy)\n",
    "    print(\"Precision:\", precision)\n",
    "    print(\"Recall:\",recall)\n",
    "    print(\"F1 Score:\",f1)\n",
    "    print()\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    outputs = model(X_test_tensor)\n",
    "    _, predicted = torch.max(outputs, 1)\n",
    "    y_true=torch.argmax(y_test_tensor, dim=1).numpy()\n",
    "    y_pred=predicted.numpy()\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    precision = precision_score(y_true, y_pred, average='weighted')\n",
    "    recall = recall_score(y_true, y_pred, average='weighted')\n",
    "    f1 = f1_score(y_true, y_pred, average='weighted')\n",
    "    print(\"Test Set:\")\n",
    "    print(\"Accuracy:\",accuracy)\n",
    "    print(\"Precision:\", precision)\n",
    "    print(\"Recall:\",recall)\n",
    "    print(\"F1 Score:\",f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.90515\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    outputs = model(X_test_tensor)\n",
    "    _, predicted = torch.max(outputs, 1)\n",
    "    y_true=torch.argmax(y_test_tensor, dim=1).numpy()\n",
    "    y_pred=predicted.numpy()\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    precision = precision_score(y_true, y_pred, average='weighted')\n",
    "    recall = recall_score(y_true, y_pred, average='weighted')\n",
    "    f1 = f1_score(y_true, y_pred, average='weighted')\n",
    "    print(\"Test Set:\")\n",
    "    print(\"Accuracy:\",accuracy)\n",
    "    print(\"Precision:\", precision)\n",
    "    print(\"Recall:\",recall)\n",
    "    print(\"F1 Score:\",f1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SVDWordEmbeddings(nn.Module):\n",
    "    def __init__(self, data, frequency_threshold=3, window_size=5, vector_size=300):\n",
    "        self.data = data\n",
    "        self.frequency_threshold = frequency_threshold\n",
    "        self.window_size = window_size\n",
    "        self.vector_size = vector_size\n",
    "        self.vocab = set()\n",
    "        self.vocab.add(PAD_TOKEN)\n",
    "        self.vocab.add(UNKNOWN_TOKEN)\n",
    "        self.sentences = []\n",
    "        self.frequency = defaultdict(int)\n",
    "        self.co_occurrence_matrix = defaultdict(int)\n",
    "        self.word_vectors = None\n",
    "\n",
    "    def preprocess_text(self, type='train'):\n",
    "        for text in self.data:\n",
    "            text = re.sub(r'[^\\w\\s\\n]', ' ', str(text).lower())\n",
    "            words = word_tokenize(text)\n",
    "            words = [START_TOKEN] + words + [END_TOKEN]\n",
    "            self.sentences.append(words)\n",
    "            for word in words:\n",
    "                self.frequency[word] += 1\n",
    "\n",
    "        if (type == 'train'):\n",
    "            for i in range(len(self.sentences)):\n",
    "                for j in range(len(self.sentences[i])):\n",
    "                    if self.frequency[self.sentences[i][j]] < self.frequency_threshold:\n",
    "                        self.sentences[i][j] = UNKNOWN_TOKEN\n",
    "\n",
    "        for sentence in self.sentences:\n",
    "            for word in sentence:\n",
    "                self.vocab.add(word)\n",
    "\n",
    "    def build_co_occurrence_matrix(self):\n",
    "        for tokens in self.sentences:\n",
    "            for i in range(len(tokens)):\n",
    "                for j in range(max(0, i - self.window_size), min(len(tokens), i + self.window_size + 1)):\n",
    "                    if i != j:\n",
    "                        self.co_occurrence_matrix[(tokens[i], tokens[j])] += 1\n",
    "\n",
    "    def apply_svd(self):\n",
    "        words = list(self.vocab)\n",
    "        words.sort()\n",
    "        self.word_index = {word: i for i, word in enumerate(words)}\n",
    "        rows, cols, data = [], [], []\n",
    "        for (word1, word2), count in self.co_occurrence_matrix.items():\n",
    "            rows.append(self.word_index[word1])\n",
    "            cols.append(self.word_index[word2])\n",
    "            data.append(count)\n",
    "        co_occurrence_matrix = csr_matrix(\n",
    "            (data, (rows, cols)), shape=(len(words), len(words)))\n",
    "        svd = TruncatedSVD(n_components=self.vector_size)\n",
    "        self.word_vectors = svd.fit_transform(co_occurrence_matrix)\n",
    "\n",
    "    def fit(self):\n",
    "        self.preprocess_text()\n",
    "        self.build_co_occurrence_matrix()\n",
    "        self.apply_svd()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from prettytable import PrettyTable\n",
    "\n",
    "table = PrettyTable()\n",
    "table.field_names = [\"Context Window Size\", \"Accuracy\", \"Precision\", \"Recall\", \"F1_Score\",\"Confusion Matrix\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 0.37380844354629517\n",
      "Epoch 2/10, Loss: 0.3816951513290405\n",
      "Epoch 3/10, Loss: 0.21368353068828583\n",
      "Epoch 4/10, Loss: 0.3267323970794678\n",
      "Epoch 5/10, Loss: 0.27925753593444824\n",
      "Epoch 6/10, Loss: 0.4329754710197449\n",
      "Epoch 7/10, Loss: 0.3113632798194885\n",
      "Epoch 8/10, Loss: 0.2926386296749115\n",
      "Epoch 9/10, Loss: 0.089718759059906\n",
      "Epoch 10/10, Loss: 0.20039130747318268\n",
      "Train Set:\n",
      "Accuracy: 0.8882083333333334\n",
      "Precision: 0.8913247614082769\n",
      "Recall: 0.8882083333333334\n",
      "F1 Score: 0.8883534000112535\n",
      "Confusion Matrix: [[25969   920  1395  1716]\n",
      " [  423 28716   322   539]\n",
      " [  909   295 24467  4329]\n",
      " [  786   215  1566 27433]]\n",
      "\n",
      "Test Set:\n",
      "Accuracy: 0.8697368421052631\n",
      "Precision: 0.8722722760788111\n",
      "Recall: 0.8697368421052631\n",
      "F1 Score: 0.86983440982138\n",
      "Confusion Matrix: [[1608   78  107  107]\n",
      " [  37 1791   29   43]\n",
      " [  69   25 1517  289]\n",
      " [  61   16  129 1694]]\n"
     ]
    }
   ],
   "source": [
    "embeddings_model_train = SVDWordEmbeddings(train_data['Description'],window_size=2)\n",
    "embeddings_model_train.fit()\n",
    "sentences_train = embeddings_model_train.sentences\n",
    "\n",
    "word_vectors_svd=embeddings_model_train.word_vectors\n",
    "word_index_svd=embeddings_model_train.word_index\n",
    "\n",
    "embeddings_model_test = SVDWordEmbeddings(test_data['Description'],window_size=2)\n",
    "embeddings_model_test.preprocess_text('test')\n",
    "sentences_test = embeddings_model_test.sentences\n",
    "\n",
    "sentence_lengths = [len(sentence) for sentence in sentences_train]\n",
    "sorted_lengths = sorted(sentence_lengths)\n",
    "index_95th_percentile = int(np.percentile(range(len(sorted_lengths)), 95))\n",
    "length_95th_percentile = sorted_lengths[index_95th_percentile]\n",
    "length_sentence = length_95th_percentile\n",
    "\n",
    "X_train = []\n",
    "for sentence in sentences_train:\n",
    "    sentence_embedding = [word_vectors_svd[word_index_svd[word]] for word in sentence]\n",
    "    if len(sentence) < length_sentence:\n",
    "        padding_needed = length_sentence - len(sentence)\n",
    "        sentence_embedding.extend(padding_needed*[word_vectors_svd[word_index_svd[PAD_TOKEN]]])\n",
    "    if sentence_embedding:\n",
    "        X_train.append((sentence_embedding[:length_sentence]))\n",
    "y_train = pd.get_dummies(train_data['Class Index'], prefix='value', dtype=int).values\n",
    "\n",
    "X_test = []\n",
    "for sentence in sentences_test:\n",
    "    sentence_embedding = [word_vectors_svd[word_index_svd.get(word, word_index_svd[UNKNOWN_TOKEN])] for word in sentence]\n",
    "    if len(sentence) < length_sentence:\n",
    "        padding_needed = length_sentence - len(sentence)\n",
    "        sentence_embedding.extend(padding_needed*[word_vectors_svd[word_index_svd[PAD_TOKEN]]])\n",
    "    if sentence_embedding:\n",
    "        X_test.append((sentence_embedding[:length_sentence]))\n",
    "y_test = pd.get_dummies(test_data['Class Index'], prefix='value', dtype=int).values\n",
    "\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.float32)\n",
    "\n",
    "class RNNClassifier(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(RNNClassifier, self).__init__()\n",
    "        self.rnn = nn.LSTM(input_size, hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        output, _ = self.rnn(x)\n",
    "        output = self.fc(output[:, -1, :])\n",
    "        return output\n",
    "\n",
    "input_size = 300\n",
    "hidden_size = 128\n",
    "output_size = 4\n",
    "model = RNNClassifier(input_size, hidden_size, output_size)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "epochs = 10\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    for batch_X, batch_y in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(batch_X)\n",
    "        loss = criterion(outputs, batch_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(f\"Epoch {epoch+1}/{epochs}, Loss: {loss.item()}\")\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    correct_samples=0\n",
    "    total_samples=0\n",
    "    for batch_X, batch_y in train_loader:\n",
    "        outputs = model(batch_X)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        y_true.extend(torch.argmax(batch_y, dim=1).numpy())\n",
    "        y_pred.extend(predicted.numpy())\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    precision = precision_score(y_true, y_pred, average='weighted')\n",
    "    recall = recall_score(y_true, y_pred, average='weighted')\n",
    "    f1 = f1_score(y_true, y_pred, average='weighted')\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    print(\"Train Set:\")\n",
    "    print(\"Accuracy:\",accuracy)\n",
    "    print(\"Precision:\", precision)\n",
    "    print(\"Recall:\",recall)\n",
    "    print(\"F1 Score:\",f1)\n",
    "    print(\"Confusion Matrix:\",cm)\n",
    "    print()\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    outputs = model(X_test_tensor)\n",
    "    _, predicted = torch.max(outputs, 1)\n",
    "    y_true=torch.argmax(y_test_tensor, dim=1).numpy()\n",
    "    y_pred=predicted.numpy()\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    precision = precision_score(y_true, y_pred, average='weighted')\n",
    "    recall = recall_score(y_true, y_pred, average='weighted')\n",
    "    f1 = f1_score(y_true, y_pred, average='weighted')\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    print(\"Test Set:\")\n",
    "    print(\"Accuracy:\",accuracy)\n",
    "    print(\"Precision:\", precision)\n",
    "    print(\"Recall:\",recall)\n",
    "    print(\"F1 Score:\",f1)\n",
    "    print(\"Confusion Matrix:\",cm)\n",
    "\n",
    "table.add_row([2,accuracy,precision,recall,f1,cm])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 0.1591673493385315\n",
      "Epoch 2/10, Loss: 0.22575244307518005\n",
      "Epoch 3/10, Loss: 0.3940945267677307\n",
      "Epoch 4/10, Loss: 0.058712538331747055\n",
      "Epoch 5/10, Loss: 0.36005276441574097\n",
      "Epoch 6/10, Loss: 0.5769515037536621\n",
      "Epoch 7/10, Loss: 0.3973618745803833\n",
      "Epoch 8/10, Loss: 0.1815149337053299\n",
      "Epoch 9/10, Loss: 0.29045355319976807\n",
      "Epoch 10/10, Loss: 0.1860528588294983\n",
      "Train Set:\n",
      "Accuracy: 0.9055083333333334\n",
      "Precision: 0.906025028929943\n",
      "Recall: 0.9055083333333334\n",
      "F1 Score: 0.9053426428469515\n",
      "Confusion Matrix: [[26899   750  1314  1037]\n",
      " [  460 29134   182   224]\n",
      " [ 1016   299 25332  3353]\n",
      " [  921   244  1539 27296]]\n",
      "\n",
      "Test Set:\n",
      "Accuracy: 0.8877631578947368\n",
      "Precision: 0.8882956075505102\n",
      "Recall: 0.8877631578947368\n",
      "F1 Score: 0.8875985193227585\n",
      "Confusion Matrix: [[1679   51   98   72]\n",
      " [  33 1818   30   19]\n",
      " [  72   28 1557  243]\n",
      " [  78   21  108 1693]]\n"
     ]
    }
   ],
   "source": [
    "embeddings_model_train = SVDWordEmbeddings(train_data['Description'],window_size=5)\n",
    "embeddings_model_train.fit()\n",
    "sentences_train = embeddings_model_train.sentences\n",
    "\n",
    "word_vectors_svd=embeddings_model_train.word_vectors\n",
    "word_index_svd=embeddings_model_train.word_index\n",
    "\n",
    "embeddings_model_test = SVDWordEmbeddings(test_data['Description'],window_size=5)\n",
    "embeddings_model_test.preprocess_text('test')\n",
    "sentences_test = embeddings_model_test.sentences\n",
    "\n",
    "sentence_lengths = [len(sentence) for sentence in sentences_train]\n",
    "sorted_lengths = sorted(sentence_lengths)\n",
    "index_95th_percentile = int(np.percentile(range(len(sorted_lengths)), 95))\n",
    "length_95th_percentile = sorted_lengths[index_95th_percentile]\n",
    "length_sentence = length_95th_percentile\n",
    "\n",
    "X_train = []\n",
    "for sentence in sentences_train:\n",
    "    sentence_embedding = [word_vectors_svd[word_index_svd[word]] for word in sentence]\n",
    "    if len(sentence) < length_sentence:\n",
    "        padding_needed = length_sentence - len(sentence)\n",
    "        sentence_embedding.extend(padding_needed*[word_vectors_svd[word_index_svd[PAD_TOKEN]]])\n",
    "    if sentence_embedding:\n",
    "        X_train.append((sentence_embedding[:length_sentence]))\n",
    "y_train = pd.get_dummies(train_data['Class Index'], prefix='value', dtype=int).values\n",
    "\n",
    "X_test = []\n",
    "for sentence in sentences_test:\n",
    "    sentence_embedding = [word_vectors_svd[word_index_svd.get(word, word_index_svd[UNKNOWN_TOKEN])] for word in sentence]\n",
    "    if len(sentence) < length_sentence:\n",
    "        padding_needed = length_sentence - len(sentence)\n",
    "        sentence_embedding.extend(padding_needed*[word_vectors_svd[word_index_svd[PAD_TOKEN]]])\n",
    "    if sentence_embedding:\n",
    "        X_test.append((sentence_embedding[:length_sentence]))\n",
    "y_test = pd.get_dummies(test_data['Class Index'], prefix='value', dtype=int).values\n",
    "\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.float32)\n",
    "\n",
    "class RNNClassifier(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(RNNClassifier, self).__init__()\n",
    "        self.rnn = nn.LSTM(input_size, hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        output, _ = self.rnn(x)\n",
    "        output = self.fc(output[:, -1, :])\n",
    "        return output\n",
    "\n",
    "input_size = 300\n",
    "hidden_size = 128\n",
    "output_size = 4\n",
    "model = RNNClassifier(input_size, hidden_size, output_size)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "epochs = 10\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    for batch_X, batch_y in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(batch_X)\n",
    "        loss = criterion(outputs, batch_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(f\"Epoch {epoch+1}/{epochs}, Loss: {loss.item()}\")\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    correct_samples=0\n",
    "    total_samples=0\n",
    "    for batch_X, batch_y in train_loader:\n",
    "        outputs = model(batch_X)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        y_true.extend(torch.argmax(batch_y, dim=1).numpy())\n",
    "        y_pred.extend(predicted.numpy())\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    precision = precision_score(y_true, y_pred, average='weighted')\n",
    "    recall = recall_score(y_true, y_pred, average='weighted')\n",
    "    f1 = f1_score(y_true, y_pred, average='weighted')\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    print(\"Train Set:\")\n",
    "    print(\"Accuracy:\",accuracy)\n",
    "    print(\"Precision:\", precision)\n",
    "    print(\"Recall:\",recall)\n",
    "    print(\"F1 Score:\",f1)\n",
    "    print(\"Confusion Matrix:\",cm)\n",
    "    print()\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    outputs = model(X_test_tensor)\n",
    "    _, predicted = torch.max(outputs, 1)\n",
    "    y_true=torch.argmax(y_test_tensor, dim=1).numpy()\n",
    "    y_pred=predicted.numpy()\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    precision = precision_score(y_true, y_pred, average='weighted')\n",
    "    recall = recall_score(y_true, y_pred, average='weighted')\n",
    "    f1 = f1_score(y_true, y_pred, average='weighted')\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    print(\"Test Set:\")\n",
    "    print(\"Accuracy:\",accuracy)\n",
    "    print(\"Precision:\", precision)\n",
    "    print(\"Recall:\",recall)\n",
    "    print(\"F1 Score:\",f1)\n",
    "    print(\"Confusion Matrix:\",cm)\n",
    "\n",
    "table.add_row([5,accuracy,precision,recall,f1,cm])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 0.37004002928733826\n",
      "Epoch 2/10, Loss: 0.4647955596446991\n",
      "Epoch 3/10, Loss: 0.2004903256893158\n",
      "Epoch 4/10, Loss: 0.14736691117286682\n",
      "Epoch 5/10, Loss: 0.1522027999162674\n",
      "Epoch 6/10, Loss: 0.4400908648967743\n",
      "Epoch 7/10, Loss: 0.34749236702919006\n",
      "Epoch 8/10, Loss: 0.25368332862854004\n",
      "Epoch 9/10, Loss: 0.35846662521362305\n",
      "Epoch 10/10, Loss: 0.29595857858657837\n",
      "Train Set:\n",
      "Accuracy: 0.9116333333333333\n",
      "Precision: 0.9126153228840616\n",
      "Recall: 0.9116333333333333\n",
      "F1 Score: 0.9116187184646952\n",
      "Confusion Matrix: [[26348   917  1729  1006]\n",
      " [  161 29510   211   118]\n",
      " [  573   216 26881  2330]\n",
      " [  635   269  2439 26657]]\n",
      "\n",
      "Test Set:\n",
      "Accuracy: 0.8957894736842106\n",
      "Precision: 0.8970712150879531\n",
      "Recall: 0.8957894736842106\n",
      "F1 Score: 0.8959381792567103\n",
      "Confusion Matrix: [[1647   59  116   78]\n",
      " [  17 1837   29   17]\n",
      " [  42   19 1669  170]\n",
      " [  49   23  173 1655]]\n"
     ]
    }
   ],
   "source": [
    "embeddings_model_train = SVDWordEmbeddings(train_data['Description'],window_size=10)\n",
    "embeddings_model_train.fit()\n",
    "sentences_train = embeddings_model_train.sentences\n",
    "\n",
    "word_vectors_svd=embeddings_model_train.word_vectors\n",
    "word_index_svd=embeddings_model_train.word_index\n",
    "\n",
    "embeddings_model_test = SVDWordEmbeddings(test_data['Description'],window_size=10)\n",
    "embeddings_model_test.preprocess_text('test')\n",
    "sentences_test = embeddings_model_test.sentences\n",
    "\n",
    "sentence_lengths = [len(sentence) for sentence in sentences_train]\n",
    "sorted_lengths = sorted(sentence_lengths)\n",
    "index_95th_percentile = int(np.percentile(range(len(sorted_lengths)), 95))\n",
    "length_95th_percentile = sorted_lengths[index_95th_percentile]\n",
    "length_sentence = length_95th_percentile\n",
    "\n",
    "X_train = []\n",
    "for sentence in sentences_train:\n",
    "    sentence_embedding = [word_vectors_svd[word_index_svd[word]] for word in sentence]\n",
    "    if len(sentence) < length_sentence:\n",
    "        padding_needed = length_sentence - len(sentence)\n",
    "        sentence_embedding.extend(padding_needed*[word_vectors_svd[word_index_svd[PAD_TOKEN]]])\n",
    "    if sentence_embedding:\n",
    "        X_train.append((sentence_embedding[:length_sentence]))\n",
    "y_train = pd.get_dummies(train_data['Class Index'], prefix='value', dtype=int).values\n",
    "\n",
    "X_test = []\n",
    "for sentence in sentences_test:\n",
    "    sentence_embedding = [word_vectors_svd[word_index_svd.get(word, word_index_svd[UNKNOWN_TOKEN])] for word in sentence]\n",
    "    if len(sentence) < length_sentence:\n",
    "        padding_needed = length_sentence - len(sentence)\n",
    "        sentence_embedding.extend(padding_needed*[word_vectors_svd[word_index_svd[PAD_TOKEN]]])\n",
    "    if sentence_embedding:\n",
    "        X_test.append((sentence_embedding[:length_sentence]))\n",
    "y_test = pd.get_dummies(test_data['Class Index'], prefix='value', dtype=int).values\n",
    "\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.float32)\n",
    "\n",
    "class RNNClassifier(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(RNNClassifier, self).__init__()\n",
    "        self.rnn = nn.LSTM(input_size, hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        output, _ = self.rnn(x)\n",
    "        output = self.fc(output[:, -1, :])\n",
    "        return output\n",
    "\n",
    "input_size = 300\n",
    "hidden_size = 128\n",
    "output_size = 4\n",
    "model = RNNClassifier(input_size, hidden_size, output_size)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "epochs = 10\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    for batch_X, batch_y in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(batch_X)\n",
    "        loss = criterion(outputs, batch_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(f\"Epoch {epoch+1}/{epochs}, Loss: {loss.item()}\")\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    correct_samples=0\n",
    "    total_samples=0\n",
    "    for batch_X, batch_y in train_loader:\n",
    "        outputs = model(batch_X)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        y_true.extend(torch.argmax(batch_y, dim=1).numpy())\n",
    "        y_pred.extend(predicted.numpy())\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    precision = precision_score(y_true, y_pred, average='weighted')\n",
    "    recall = recall_score(y_true, y_pred, average='weighted')\n",
    "    f1 = f1_score(y_true, y_pred, average='weighted')\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    print(\"Train Set:\")\n",
    "    print(\"Accuracy:\",accuracy)\n",
    "    print(\"Precision:\", precision)\n",
    "    print(\"Recall:\",recall)\n",
    "    print(\"F1 Score:\",f1)\n",
    "    print(\"Confusion Matrix:\",cm)\n",
    "    print()\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    outputs = model(X_test_tensor)\n",
    "    _, predicted = torch.max(outputs, 1)\n",
    "    y_true=torch.argmax(y_test_tensor, dim=1).numpy()\n",
    "    y_pred=predicted.numpy()\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    precision = precision_score(y_true, y_pred, average='weighted')\n",
    "    recall = recall_score(y_true, y_pred, average='weighted')\n",
    "    f1 = f1_score(y_true, y_pred, average='weighted')\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    print(\"Test Set:\")\n",
    "    print(\"Accuracy:\",accuracy)\n",
    "    print(\"Precision:\", precision)\n",
    "    print(\"Recall:\",recall)\n",
    "    print(\"F1 Score:\",f1)\n",
    "    print(\"Confusion Matrix:\",cm)\n",
    "\n",
    "table.add_row([10,accuracy,precision,recall,f1,cm])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------+--------------------+--------------------+--------------------+--------------------+-------------------------+\n",
      "| Context Window Size |      Accuracy      |     Precision      |       Recall       |      F1_Score      |     Confusion Matrix    |\n",
      "+---------------------+--------------------+--------------------+--------------------+--------------------+-------------------------+\n",
      "|          2          | 0.8697368421052631 | 0.8722722760788111 | 0.8697368421052631 |  0.86983440982138  |  [[1608   78  107  107] |\n",
      "|                     |                    |                    |                    |                    |   [  37 1791   29   43] |\n",
      "|                     |                    |                    |                    |                    |   [  69   25 1517  289] |\n",
      "|                     |                    |                    |                    |                    |  [  61   16  129 1694]] |\n",
      "|          5          | 0.8877631578947368 | 0.8882956075505102 | 0.8877631578947368 | 0.8875985193227585 |  [[1679   51   98   72] |\n",
      "|                     |                    |                    |                    |                    |   [  33 1818   30   19] |\n",
      "|                     |                    |                    |                    |                    |   [  72   28 1557  243] |\n",
      "|                     |                    |                    |                    |                    |  [  78   21  108 1693]] |\n",
      "|          10         | 0.8957894736842106 | 0.8970712150879531 | 0.8957894736842106 | 0.8959381792567103 |  [[1647   59  116   78] |\n",
      "|                     |                    |                    |                    |                    |   [  17 1837   29   17] |\n",
      "|                     |                    |                    |                    |                    |   [  42   19 1669  170] |\n",
      "|                     |                    |                    |                    |                    |  [  49   23  173 1655]] |\n",
      "+---------------------+--------------------+--------------------+--------------------+--------------------+-------------------------+\n"
     ]
    }
   ],
   "source": [
    "print(table)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
