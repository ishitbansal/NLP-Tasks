{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "from collections import defaultdict\n",
    "from scipy.sparse import csr_matrix\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "\n",
    "UNKNOWN_TOKEN='UNK'\n",
    "PAD_TOKEN='PAD'\n",
    "START_TOKEN = '<START>'\n",
    "END_TOKEN = '<END>'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data=pd.read_csv('./train.csv')\n",
    "test_data=pd.read_csv('./test.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(data,type='train'):\n",
    "    sentences=[]\n",
    "    vocab=set()\n",
    "    vocab.add(PAD_TOKEN)\n",
    "    vocab.add(UNKNOWN_TOKEN)\n",
    "    total=0\n",
    "\n",
    "    frequency=dict()\n",
    "    for text in data:\n",
    "        text = re.sub(r'[^\\w\\s\\n]', ' ', str(text).lower())\n",
    "        words = word_tokenize(text)\n",
    "        words = [START_TOKEN] + words + [END_TOKEN]\n",
    "        sentences.append(words)\n",
    "        for word in words:\n",
    "            frequency[word]=frequency.get(word,0)+1\n",
    "            total+=1\n",
    "    \n",
    "    if type=='train':\n",
    "        frequency_threshold=3\n",
    "        for i in range(len(sentences)):\n",
    "            for j in range(len(sentences[i])):\n",
    "                if frequency[sentences[i][j]]<frequency_threshold:\n",
    "                    sentences[i][j]=UNKNOWN_TOKEN\n",
    "\n",
    "    for sentence in sentences:\n",
    "        for word in sentence:\n",
    "            vocab.add(word)\n",
    "    vocab=list(vocab)\n",
    "    return sentences,vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_train,vocab = preprocess_text(train_data['Description'])\n",
    "sentences_test,_ = preprocess_text(test_data['Description'],'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32009\n"
     ]
    }
   ],
   "source": [
    "print(len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word2ind = {}\n",
    "# ind2word = {}\n",
    "\n",
    "# for ind, word in enumerate(vocab):\n",
    "#     word2ind[word] = ind\n",
    "#     ind2word[ind] = word\n",
    "\n",
    "# for i in range(len(sentences_train)):\n",
    "#     sentences_train[i]=[word2ind[word] for word in sentences_train[i]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_pairs(corpus,window_size=5):\n",
    "#     word_pairs=[]\n",
    "#     for sentence in corpus:\n",
    "#         for i in range(len(sentence)):\n",
    "#             for j in range(max(0,i-window_size),min(i+window_size+1,len(sentence))):\n",
    "#                 if(i!=j):\n",
    "#                     word_pairs.append((sentence[i],sentence[j]))\n",
    "#     return word_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Loss = 2.5585875701939442\n",
      "Epoch 2: Loss = 1.8237238593426548\n",
      "Epoch 3: Loss = 1.786290263232018\n",
      "Epoch 4: Loss = 1.7433882371003326\n",
      "Epoch 5: Loss = 1.7090182398404352\n",
      "Epoch 6: Loss = 1.6788035031284985\n",
      "Epoch 7: Loss = 1.646608303452957\n",
      "Epoch 8: Loss = 1.608922379434993\n",
      "Epoch 9: Loss = 1.5642176757106727\n",
      "Epoch 10: Loss = 1.514336836986116\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "import random\n",
    "    \n",
    "class SkipGramNegativeSampling:\n",
    "    def __init__(self, corpus, vector_size=300, window_size=5, negative_samples=5, learning_rate=0.025):\n",
    "        self.corpus = corpus\n",
    "        self.vector_size = vector_size\n",
    "        self.window_size = window_size\n",
    "        self.negative_samples = negative_samples\n",
    "        self.learning_rate = learning_rate\n",
    "        self.word2id = {}\n",
    "        self.id2word = {}\n",
    "        self.vocab_size = 0\n",
    "        self.word_count = defaultdict(int)\n",
    "        self.word_pairs = []\n",
    "        self.initialize()\n",
    "\n",
    "    def initialize(self):\n",
    "        for sentence in self.corpus:\n",
    "            for word in sentence:\n",
    "                self.word_count[word] += 1\n",
    "        sorted_vocab = sorted(self.word_count, key=self.word_count.get, reverse=True)\n",
    "        for i, word in enumerate(sorted_vocab):\n",
    "            self.word2id[word] = i\n",
    "            self.id2word[i] = word\n",
    "        self.vocab_size = len(self.word2id)\n",
    "\n",
    "        threshold=1e-5\n",
    "        negative_subsamples=[]\n",
    "        for word, freq in self.word_count.items():\n",
    "            keep_prob = (np.sqrt(freq / threshold) + 1) * (threshold / freq)\n",
    "            if random.random() < keep_prob:\n",
    "                negative_subsamples.extend([word] * self.word_count[word])\n",
    "\n",
    "    def similarity(self, vec1, vec2):\n",
    "        dot_product = np.dot(vec1, vec2)\n",
    "        return dot_product\n",
    "    \n",
    "\n",
    "    def generate_word_pairs(self):\n",
    "        word_pairs=set()\n",
    "        for sentence in self.corpus:\n",
    "            for i, target_word in enumerate(sentence):\n",
    "                target_word_id = self.word2id[target_word]\n",
    "                for j in range(i - self.window_size, i + self.window_size + 1):\n",
    "                    if j != i and j >= 0 and j < len(sentence):\n",
    "                        context_word = sentence[j]\n",
    "                        context_word_id = self.word2id[context_word]\n",
    "                        word_pairs.add((target_word_id, context_word_id))\n",
    "        self.word_pairs=list(word_pairs)\n",
    "        random.shuffle(self.word_pairs)\n",
    "\n",
    "    def train(self, epochs=5):\n",
    "        self.generate_word_pairs()\n",
    "        self.W = np.random.uniform(-0.5/self.vector_size, 0.5/self.vector_size, (self.vocab_size, self.vector_size))\n",
    "        self.C = np.random.uniform(-0.5/self.vector_size, 0.5/self.vector_size, (self.vocab_size, self.vector_size))\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            loss = 0\n",
    "            for target_word_id, context_word_id in self.word_pairs:\n",
    "                loss += self.train_pair(target_word_id, context_word_id)\n",
    "            print(f\"Epoch {epoch + 1}: Loss = {loss / len(self.word_pairs)}\")\n",
    "    \n",
    "    def train_pair(self, target_word_id, context_word_id):\n",
    "        target_vector = self.W[target_word_id] # w\n",
    "        context_vector = self.C[context_word_id] # c_pos\n",
    "\n",
    "        negative_samples = random.choices(range(self.vocab_size), k=self.negative_samples)\n",
    "\n",
    "        similarity_score = self.similarity(target_vector, context_vector) # w.c_pos\n",
    "        sigmoid_score = self.sigmoid(similarity_score) # sigmoid(w.c_pos)\n",
    "        gradients = (sigmoid_score - 1) * context_vector \n",
    "        self.C[context_word_id]-=self.learning_rate *(sigmoid_score - 1) * target_vector # C^(t+1)_pos=C^(t)_pos-lr*(sigmoid(c_pos.w)-1)*w\n",
    "        loss=-np.log(sigmoid_score+1e-10) # log(sigmoid(c_pos.w))\n",
    "\n",
    "        for sample_word_id in negative_samples :\n",
    "            sample_vector = self.C[sample_word_id] # c_neg\n",
    "            similarity_score = self.similarity(target_vector, sample_vector) # w.c_neg\n",
    "            sigmoid_score = self.sigmoid(similarity_score) # sigmoid(w.c_neg)\n",
    "            gradients += sigmoid_score * sample_vector\n",
    "            self.C[sample_word_id]-=self.learning_rate*sigmoid_score*target_vector # C^(t+1)_neg=C^(t)_neg-lr*sigmoid(c_neg.w)*w\n",
    "            loss+=-np.log(1-sigmoid_score+1e-10) # log(1-sigmoid(c_neg.w))\n",
    "        \n",
    "        self.W[target_word_id] -= self.learning_rate * gradients\n",
    "        return loss\n",
    "    \n",
    "    def get_word_vector(self, word):\n",
    "        if word in self.word2id:\n",
    "            return self.W[self.word2id[word]]\n",
    "        else:\n",
    "            return self.W[self.word2id[UNKNOWN_TOKEN]]\n",
    "\n",
    "    def sigmoid(self,z):\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "\n",
    "corpus=sentences_train\n",
    "sg_model = SkipGramNegativeSampling(corpus)\n",
    "sg_model.train(epochs=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_lengths = [len(sentence) for sentence in sentences_train]\n",
    "sorted_lengths = sorted(sentence_lengths)\n",
    "index_95th_percentile = int(np.percentile(range(len(sorted_lengths)), 95))\n",
    "length_95th_percentile = sorted_lengths[index_95th_percentile]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "length_sentence=length_95th_percentile\n",
    "\n",
    "X_train = []\n",
    "for sentence in sentences_train:\n",
    "    sentence_embedding = [sg_model.get_word_vector(word) for word in sentence]\n",
    "    if len(sentence)<length_sentence:\n",
    "        padding_needed = length_sentence - len(sentence)\n",
    "        sentence_embedding.extend(padding_needed*[sg_model.get_word_vector(PAD_TOKEN)])\n",
    "    if sentence_embedding:\n",
    "        X_train.append((sentence_embedding[:length_sentence]))\n",
    "y_train = pd.get_dummies(train_data['Class Index'], prefix='value', dtype=int).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = []\n",
    "for sentence in sentences_test:\n",
    "    sentence_embedding = [sg_model.get_word_vector(word) for word in sentence]\n",
    "    if len(sentence)<length_sentence:\n",
    "        padding_needed = length_sentence - len(sentence)\n",
    "        sentence_embedding.extend(padding_needed*[sg_model.get_word_vector(PAD_TOKEN)])\n",
    "    if sentence_embedding:\n",
    "        # sentence_embedding = np.array(sentence_embedding[:length_sentence]) \n",
    "        X_test.append((sentence_embedding[:length_sentence]))\n",
    "y_test = pd.get_dummies(test_data['Class Index'], prefix='value', dtype=int).values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNClassifier(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(RNNClassifier, self).__init__()\n",
    "        self.rnn = nn.LSTM(input_size, hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        output, _ = self.rnn(x)\n",
    "        output = self.fc(output[:, -1, :]) \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_509498/3299132171.py:9: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:275.)\n",
      "  X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n"
     ]
    }
   ],
   "source": [
    "input_size = 300  \n",
    "hidden_size = 128\n",
    "output_size = 4\n",
    "model = RNNClassifier(input_size, hidden_size, output_size)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 0.27671459317207336\n",
      "Epoch 2/10, Loss: 0.16225211322307587\n",
      "Epoch 3/10, Loss: 0.14152804017066956\n",
      "Epoch 4/10, Loss: 0.26288023591041565\n",
      "Epoch 5/10, Loss: 0.06347747147083282\n",
      "Epoch 6/10, Loss: 0.1854410469532013\n",
      "Epoch 7/10, Loss: 0.025034695863723755\n",
      "Epoch 8/10, Loss: 0.08606728911399841\n",
      "Epoch 9/10, Loss: 0.10629647970199585\n",
      "Epoch 10/10, Loss: 0.1343078911304474\n"
     ]
    }
   ],
   "source": [
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "epochs = 10\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    for batch_X, batch_y in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(batch_X)\n",
    "        loss = criterion(outputs, batch_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(f\"Epoch {epoch+1}/{epochs}, Loss: {loss.item()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9055263157894737\n",
      "Precision: 0.9057442508427023\n",
      "Recall: 0.9055263157894737\n",
      "F1 Score: 0.9055254208469927\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    outputs = model(X_test_tensor)\n",
    "    _, predicted = torch.max(outputs, 1)\n",
    "    y_true=torch.argmax(y_test_tensor, dim=1).numpy()\n",
    "    y_pred=predicted.numpy()\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    precision = precision_score(y_true, y_pred, average='weighted')\n",
    "    recall = recall_score(y_true, y_pred, average='weighted')\n",
    "    f1 = f1_score(y_true, y_pred, average='weighted')\n",
    "\n",
    "    print(\"Accuracy:\",accuracy)\n",
    "    print(\"Precision:\", precision)\n",
    "    print(\"Recall:\",recall)\n",
    "    print(\"F1 Score:\",f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "y_true = []\n",
    "y_pred = []\n",
    "correct_samples=0\n",
    "total_samples=0\n",
    "for batch_X, batch_y in train_loader:\n",
    "    outputs = model(batch_X)\n",
    "    _, predicted = torch.max(outputs, 1)\n",
    "    y_true.extend(torch.argmax(batch_y, dim=1).numpy())\n",
    "    y_pred.extend(predicted.numpy())\n",
    "\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "precision = precision_score(y_true, y_pred, average='weighted')\n",
    "recall = recall_score(y_true, y_pred, average='weighted')\n",
    "f1 = f1_score(y_true, y_pred, average='weighted')\n",
    "\n",
    "print(\"Accuracy:\",accuracy)\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\",recall)\n",
    "print(\"F1 Score:\",f1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from collections import defaultdict\n",
    "import random\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "PAD_TOKEN = '<PAD>'\n",
    "UNKNOWN_TOKEN = '<UNKNOWN>'\n",
    "START_TOKEN = '<START>'\n",
    "END_TOKEN = '<END>'\n",
    "\n",
    "def preprocess_text(data,type='train'):\n",
    "    sentences=[]\n",
    "    frequency=dict()\n",
    "    for text in data:\n",
    "        text = re.sub(r'[^\\w\\s\\n]', ' ', str(text).lower())\n",
    "        words = word_tokenize(text)\n",
    "        words = [START_TOKEN] + words + [END_TOKEN]\n",
    "        sentences.append(words)\n",
    "        for word in words:\n",
    "            frequency[word]=frequency.get(word,0)+1\n",
    "    \n",
    "    if type=='train':\n",
    "        frequency_threshold=3\n",
    "        for i in range(len(sentences)):\n",
    "            for j in range(len(sentences[i])):\n",
    "                if frequency[sentences[i][j]]<frequency_threshold:\n",
    "                    sentences[i][j]=UNKNOWN_TOKEN\n",
    "    return sentences\n",
    "\n",
    "class SkipGramNegativeSampling(nn.Module):\n",
    "    def __init__(self, corpus, vector_size=300, window_size=5, negative_samples=5, learning_rate=0.025):\n",
    "        self.corpus = corpus\n",
    "        self.vector_size = vector_size\n",
    "        self.window_size = window_size\n",
    "        self.negative_samples = negative_samples\n",
    "        self.learning_rate = learning_rate\n",
    "        self.word2id = {}\n",
    "        self.id2word = {}\n",
    "        self.vocab_size = 0\n",
    "        self.word_count = defaultdict(int)\n",
    "        self.word_pairs = []\n",
    "        self.vocab=set()\n",
    "        self.initialize()\n",
    "\n",
    "    def initialize(self):\n",
    "        for sentence in self.corpus:\n",
    "            for word in sentence:\n",
    "                self.word_count[word] += 1\n",
    "                self.vocab.add(word)\n",
    "        self.vocab.add(PAD_TOKEN)\n",
    "        self.word_count[PAD_TOKEN]=1\n",
    "        words = list(self.vocab)\n",
    "        words.sort()\n",
    "        for i, word in enumerate(words):\n",
    "            self.word2id[word] = i\n",
    "            self.id2word[i] = word\n",
    "        self.vocab_size = len(self.vocab)\n",
    "\n",
    "    def similarity(self, vec1, vec2):\n",
    "        dot_product = np.dot(vec1, vec2)\n",
    "        return dot_product\n",
    "    \n",
    "    def sigmoid(self,z):\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "\n",
    "    def generate_word_pairs(self):\n",
    "        word_pairs=set()\n",
    "        for sentence in self.corpus:\n",
    "            for i, target_word in enumerate(sentence):\n",
    "                target_word_id = self.word2id[target_word]\n",
    "                for j in range(i - self.window_size, i + self.window_size + 1):\n",
    "                    if j != i and j >= 0 and j < len(sentence):\n",
    "                        context_word = sentence[j]\n",
    "                        context_word_id = self.word2id[context_word]\n",
    "                        word_pairs.add((target_word_id, context_word_id))\n",
    "        self.word_pairs=list(word_pairs)\n",
    "        random.shuffle(self.word_pairs)\n",
    "\n",
    "    def train(self, epochs=5):\n",
    "        self.generate_word_pairs()\n",
    "        self.W = np.random.uniform(-0.5/self.vector_size, 0.5/self.vector_size, (self.vocab_size, self.vector_size))\n",
    "        self.C = np.random.uniform(-0.5/self.vector_size, 0.5/self.vector_size, (self.vocab_size, self.vector_size))\n",
    "        for epoch in range(epochs):\n",
    "            loss = 0\n",
    "            for target_word_id, context_word_id in self.word_pairs:\n",
    "                loss += self.train_pair(target_word_id, context_word_id)\n",
    "            print(f\"Epoch {epoch + 1}: Loss = {loss / len(self.word_pairs)}\")\n",
    "\n",
    "    def train_pair(self, target_word_id, context_word_id):\n",
    "        target_vector = self.W[target_word_id] # w\n",
    "        context_vector = self.C[context_word_id] # c_pos\n",
    "\n",
    "        negative_samples = random.choices(range(self.vocab_size), k=self.negative_samples)\n",
    "\n",
    "        similarity_score = self.similarity(target_vector, context_vector) # w.c_pos\n",
    "        sigmoid_score = self.sigmoid(similarity_score) # sigmoid(w.c_pos)\n",
    "        gradients = (sigmoid_score - 1) * context_vector \n",
    "        self.C[context_word_id]-=self.learning_rate *(sigmoid_score - 1) * target_vector # C^(t+1)_pos=C^(t)_pos-lr*(sigmoid(c_pos.w)-1)*w\n",
    "        loss=-np.log(sigmoid_score+1e-10) # log(sigmoid(c_pos.w))\n",
    "\n",
    "        for sample_word_id in negative_samples :\n",
    "            sample_vector = self.C[sample_word_id] # c_neg\n",
    "            similarity_score = self.similarity(target_vector, sample_vector) # w.c_neg\n",
    "            sigmoid_score = self.sigmoid(similarity_score) # sigmoid(w.c_neg)\n",
    "            gradients += sigmoid_score * sample_vector\n",
    "            self.C[sample_word_id]-=self.learning_rate*sigmoid_score*target_vector # C^(t+1)_neg=C^(t)_neg-lr*sigmoid(c_neg.w)*w\n",
    "            loss+=-np.log(1-sigmoid_score+1e-10) # log(1-sigmoid(c_neg.w))\n",
    "        \n",
    "        self.W[target_word_id] -= self.learning_rate * gradients\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from prettytable import PrettyTable\n",
    "\n",
    "table = PrettyTable()\n",
    "table.field_names = [\"Context Window Size\", \"Accuracy\", \"Precision\", \"Recall\", \"F1_Score\",\"Confusion Matrix\"]\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Loss = 3.0768120968836263\n",
      "Epoch 2: Loss = 1.846027502287442\n",
      "Epoch 3: Loss = 1.7637964924873073\n",
      "Epoch 4: Loss = 1.7382735792856723\n",
      "Epoch 5: Loss = 1.706115747239165\n",
      "Epoch 6: Loss = 1.6677122976852459\n",
      "Epoch 7: Loss = 1.6317361879417591\n",
      "Epoch 8: Loss = 1.5953168564744111\n",
      "Epoch 9: Loss = 1.5557750605671734\n",
      "Epoch 10: Loss = 1.511397127347483\n",
      "Epoch 1/10, Loss: 0.661766529083252\n",
      "Epoch 2/10, Loss: 0.1255021095275879\n",
      "Epoch 3/10, Loss: 0.1881602704524994\n",
      "Epoch 4/10, Loss: 0.3539079427719116\n",
      "Epoch 5/10, Loss: 0.17351451516151428\n",
      "Epoch 6/10, Loss: 0.3443686068058014\n",
      "Epoch 7/10, Loss: 0.16960179805755615\n",
      "Epoch 8/10, Loss: 0.07058196514844894\n",
      "Epoch 9/10, Loss: 0.06119907274842262\n",
      "Epoch 10/10, Loss: 0.17157040536403656\n",
      "Train Set:\n",
      "Accuracy: 0.9497416666666667\n",
      "Precision: 0.9508833645462933\n",
      "Recall: 0.9497416666666667\n",
      "F1 Score: 0.9496591164105659\n",
      "Confusion Matrix: [[28438   304   355   903]\n",
      " [  103 29787    20    90]\n",
      " [  699   229 26778  2294]\n",
      " [  267    74   693 28966]]\n",
      "\n",
      "Test Set:\n",
      "Accuracy: 0.8953947368421052\n",
      "Precision: 0.8969339297630881\n",
      "Recall: 0.8953947368421052\n",
      "F1 Score: 0.8952788674747271\n",
      "Confusion Matrix: [[1701   45   54  100]\n",
      " [  26 1835   14   25]\n",
      " [  82   21 1560  237]\n",
      " [  58   28  105 1709]]\n"
     ]
    }
   ],
   "source": [
    "train_data=pd.read_csv('./train.csv')\n",
    "sentences_train = preprocess_text(train_data['Description'])\n",
    "sg_model = SkipGramNegativeSampling(sentences_train,window_size=2)\n",
    "sg_model.train(epochs=10)\n",
    "word_vectors_sg=sg_model.W\n",
    "word_index_sg=sg_model.word2id\n",
    "\n",
    "test_data = pd.read_csv('./test.csv')\n",
    "\n",
    "sentences_train= preprocess_text(train_data['Description'])\n",
    "sentences_test = preprocess_text(test_data['Description'],'test')\n",
    "\n",
    "sentence_lengths = [len(sentence) for sentence in sentences_train]\n",
    "sorted_lengths = sorted(sentence_lengths)\n",
    "index_95th_percentile = int(np.percentile(range(len(sorted_lengths)), 95))\n",
    "length_95th_percentile = sorted_lengths[index_95th_percentile]\n",
    "length_sentence = length_95th_percentile\n",
    "\n",
    "X_train = []\n",
    "for sentence in sentences_train:\n",
    "    sentence_embedding = [word_vectors_sg[word_index_sg[word]] for word in sentence]\n",
    "    if len(sentence) < length_sentence:\n",
    "        padding_needed = length_sentence - len(sentence)\n",
    "        sentence_embedding.extend(padding_needed*[word_vectors_sg[word_index_sg[PAD_TOKEN]]])\n",
    "    if sentence_embedding:\n",
    "        X_train.append((sentence_embedding[:length_sentence]))\n",
    "y_train = pd.get_dummies(train_data['Class Index'], prefix='value', dtype=int).values\n",
    "\n",
    "X_test = []\n",
    "for sentence in sentences_test:\n",
    "    sentence_embedding = [word_vectors_sg[word_index_sg.get(word, word_index_sg[UNKNOWN_TOKEN])] for word in sentence]\n",
    "    if len(sentence) < length_sentence:\n",
    "        padding_needed = length_sentence - len(sentence)\n",
    "        sentence_embedding.extend(padding_needed*[word_vectors_sg[word_index_sg[PAD_TOKEN]]])\n",
    "    if sentence_embedding:\n",
    "        X_test.append((sentence_embedding[:length_sentence]))\n",
    "y_test = pd.get_dummies(test_data['Class Index'], prefix='value', dtype=int).values\n",
    "\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.float32)\n",
    "\n",
    "class RNNClassifier(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(RNNClassifier, self).__init__()\n",
    "        self.rnn = nn.LSTM(input_size, hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        output, _ = self.rnn(x)\n",
    "        output = self.fc(output[:, -1, :]) \n",
    "        return output\n",
    "    \n",
    "input_size = 300  \n",
    "hidden_size = 128\n",
    "output_size = 4\n",
    "model = RNNClassifier(input_size, hidden_size, output_size)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "epochs = 10\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    for batch_X, batch_y in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(batch_X)\n",
    "        loss = criterion(outputs, batch_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(f\"Epoch {epoch+1}/{epochs}, Loss: {loss.item()}\")\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    correct_samples=0\n",
    "    total_samples=0\n",
    "    for batch_X, batch_y in train_loader:\n",
    "        outputs = model(batch_X)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        y_true.extend(torch.argmax(batch_y, dim=1).numpy())\n",
    "        y_pred.extend(predicted.numpy())\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    precision = precision_score(y_true, y_pred, average='weighted')\n",
    "    recall = recall_score(y_true, y_pred, average='weighted')\n",
    "    f1 = f1_score(y_true, y_pred, average='weighted')\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    print(\"Train Set:\")\n",
    "    print(\"Accuracy:\",accuracy)\n",
    "    print(\"Precision:\", precision)\n",
    "    print(\"Recall:\",recall)\n",
    "    print(\"F1 Score:\",f1)\n",
    "    print(\"Confusion Matrix:\",cm)\n",
    "    print()\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    outputs = model(X_test_tensor)\n",
    "    _, predicted = torch.max(outputs, 1)\n",
    "    y_true=torch.argmax(y_test_tensor, dim=1).numpy()\n",
    "    y_pred=predicted.numpy()\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    precision = precision_score(y_true, y_pred, average='weighted')\n",
    "    recall = recall_score(y_true, y_pred, average='weighted')\n",
    "    f1 = f1_score(y_true, y_pred, average='weighted')\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    print(\"Test Set:\")\n",
    "    print(\"Accuracy:\",accuracy)\n",
    "    print(\"Precision:\", precision)\n",
    "    print(\"Recall:\",recall)\n",
    "    print(\"F1 Score:\",f1)\n",
    "    print(\"Confusion Matrix:\",cm)\n",
    "\n",
    "table.add_row([2,accuracy,precision,recall,f1,cm])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Loss = 2.5658881703760787\n",
      "Epoch 2: Loss = 1.8236889297626786\n",
      "Epoch 3: Loss = 1.786828297251044\n",
      "Epoch 4: Loss = 1.7432736420894277\n",
      "Epoch 5: Loss = 1.7096412387247542\n",
      "Epoch 6: Loss = 1.679041613918702\n",
      "Epoch 7: Loss = 1.6467564590376735\n",
      "Epoch 8: Loss = 1.6090533694581557\n",
      "Epoch 9: Loss = 1.5642193514899343\n",
      "Epoch 10: Loss = 1.5135694702942455\n",
      "Epoch 1/10, Loss: 0.15511079132556915\n",
      "Epoch 2/10, Loss: 0.1891205757856369\n",
      "Epoch 3/10, Loss: 0.06072907894849777\n",
      "Epoch 4/10, Loss: 0.1574212610721588\n",
      "Epoch 5/10, Loss: 0.19283118844032288\n",
      "Epoch 6/10, Loss: 0.15719085931777954\n",
      "Epoch 7/10, Loss: 0.12699761986732483\n",
      "Epoch 8/10, Loss: 0.08932410925626755\n",
      "Epoch 9/10, Loss: 0.032409679144620895\n",
      "Epoch 10/10, Loss: 0.21677207946777344\n",
      "Train Set:\n",
      "Accuracy: 0.9677916666666667\n",
      "Precision: 0.9691314022270019\n",
      "Recall: 0.9677916666666667\n",
      "F1 Score: 0.967825186834218\n",
      "Confusion Matrix: [[29103   136   271   490]\n",
      " [   80 29842    22    56]\n",
      " [  232    36 27540  2192]\n",
      " [  128    23   199 29650]]\n",
      "\n",
      "Test Set:\n",
      "Accuracy: 0.9073684210526316\n",
      "Precision: 0.9094448803242379\n",
      "Recall: 0.9073684210526316\n",
      "F1 Score: 0.907279656801976\n",
      "Confusion Matrix: [[1719   35   61   85]\n",
      " [  22 1842   19   17]\n",
      " [  75   12 1570  243]\n",
      " [  47   12   76 1765]]\n"
     ]
    }
   ],
   "source": [
    "train_data=pd.read_csv('./train.csv')\n",
    "sentences_train = preprocess_text(train_data['Description'])\n",
    "sg_model = SkipGramNegativeSampling(sentences_train,window_size=5)\n",
    "sg_model.train(epochs=10)\n",
    "word_vectors_sg=sg_model.W\n",
    "word_index_sg=sg_model.word2id\n",
    "\n",
    "test_data = pd.read_csv('./test.csv')\n",
    "\n",
    "sentences_train= preprocess_text(train_data['Description'])\n",
    "sentences_test = preprocess_text(test_data['Description'],'test')\n",
    "\n",
    "sentence_lengths = [len(sentence) for sentence in sentences_train]\n",
    "sorted_lengths = sorted(sentence_lengths)\n",
    "index_95th_percentile = int(np.percentile(range(len(sorted_lengths)), 95))\n",
    "length_95th_percentile = sorted_lengths[index_95th_percentile]\n",
    "length_sentence = length_95th_percentile\n",
    "\n",
    "X_train = []\n",
    "for sentence in sentences_train:\n",
    "    sentence_embedding = [word_vectors_sg[word_index_sg[word]] for word in sentence]\n",
    "    if len(sentence) < length_sentence:\n",
    "        padding_needed = length_sentence - len(sentence)\n",
    "        sentence_embedding.extend(padding_needed*[word_vectors_sg[word_index_sg[PAD_TOKEN]]])\n",
    "    if sentence_embedding:\n",
    "        X_train.append((sentence_embedding[:length_sentence]))\n",
    "y_train = pd.get_dummies(train_data['Class Index'], prefix='value', dtype=int).values\n",
    "\n",
    "X_test = []\n",
    "for sentence in sentences_test:\n",
    "    sentence_embedding = [word_vectors_sg[word_index_sg.get(word, word_index_sg[UNKNOWN_TOKEN])] for word in sentence]\n",
    "    if len(sentence) < length_sentence:\n",
    "        padding_needed = length_sentence - len(sentence)\n",
    "        sentence_embedding.extend(padding_needed*[word_vectors_sg[word_index_sg[PAD_TOKEN]]])\n",
    "    if sentence_embedding:\n",
    "        X_test.append((sentence_embedding[:length_sentence]))\n",
    "y_test = pd.get_dummies(test_data['Class Index'], prefix='value', dtype=int).values\n",
    "\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.float32)\n",
    "\n",
    "class RNNClassifier(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(RNNClassifier, self).__init__()\n",
    "        self.rnn = nn.LSTM(input_size, hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        output, _ = self.rnn(x)\n",
    "        output = self.fc(output[:, -1, :]) \n",
    "        return output\n",
    "    \n",
    "input_size = 300  \n",
    "hidden_size = 128\n",
    "output_size = 4\n",
    "model = RNNClassifier(input_size, hidden_size, output_size)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "epochs = 10\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    for batch_X, batch_y in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(batch_X)\n",
    "        loss = criterion(outputs, batch_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(f\"Epoch {epoch+1}/{epochs}, Loss: {loss.item()}\")\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    correct_samples=0\n",
    "    total_samples=0\n",
    "    for batch_X, batch_y in train_loader:\n",
    "        outputs = model(batch_X)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        y_true.extend(torch.argmax(batch_y, dim=1).numpy())\n",
    "        y_pred.extend(predicted.numpy())\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    precision = precision_score(y_true, y_pred, average='weighted')\n",
    "    recall = recall_score(y_true, y_pred, average='weighted')\n",
    "    f1 = f1_score(y_true, y_pred, average='weighted')\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    print(\"Train Set:\")\n",
    "    print(\"Accuracy:\",accuracy)\n",
    "    print(\"Precision:\", precision)\n",
    "    print(\"Recall:\",recall)\n",
    "    print(\"F1 Score:\",f1)\n",
    "    print(\"Confusion Matrix:\",cm)\n",
    "    print()\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    outputs = model(X_test_tensor)\n",
    "    _, predicted = torch.max(outputs, 1)\n",
    "    y_true=torch.argmax(y_test_tensor, dim=1).numpy()\n",
    "    y_pred=predicted.numpy()\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    precision = precision_score(y_true, y_pred, average='weighted')\n",
    "    recall = recall_score(y_true, y_pred, average='weighted')\n",
    "    f1 = f1_score(y_true, y_pred, average='weighted')\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    print(\"Test Set:\")\n",
    "    print(\"Accuracy:\",accuracy)\n",
    "    print(\"Precision:\", precision)\n",
    "    print(\"Recall:\",recall)\n",
    "    print(\"F1 Score:\",f1)\n",
    "    print(\"Confusion Matrix:\",cm)\n",
    "\n",
    "table.add_row([10,accuracy,precision,recall,f1,cm])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Loss = 2.37532437156803\n",
      "Epoch 2: Loss = 1.8549118276662764\n",
      "Epoch 3: Loss = 1.8025203860644239\n",
      "Epoch 4: Loss = 1.765109287291478\n",
      "Epoch 5: Loss = 1.734151278843685\n",
      "Epoch 6: Loss = 1.703118658300197\n",
      "Epoch 7: Loss = 1.6681055772703846\n",
      "Epoch 8: Loss = 1.62701956210864\n",
      "Epoch 9: Loss = 1.5812277136384694\n",
      "Epoch 10: Loss = 1.5341814429360627\n",
      "Epoch 1/10, Loss: 0.5076616406440735\n",
      "Epoch 2/10, Loss: 0.5257648229598999\n",
      "Epoch 3/10, Loss: 0.29216882586479187\n",
      "Epoch 4/10, Loss: 0.10397431999444962\n",
      "Epoch 5/10, Loss: 0.27815452218055725\n",
      "Epoch 6/10, Loss: 0.0899357795715332\n",
      "Epoch 7/10, Loss: 0.057357918471097946\n",
      "Epoch 8/10, Loss: 0.05821150168776512\n",
      "Epoch 9/10, Loss: 0.11028043925762177\n",
      "Epoch 10/10, Loss: 0.05545838177204132\n",
      "Train Set:\n",
      "Accuracy: 0.9737083333333333\n",
      "Precision: 0.9742444474359087\n",
      "Recall: 0.9737083333333333\n",
      "F1 Score: 0.9737964490106757\n",
      "Confusion Matrix: [[29020    89   565   326]\n",
      " [   97 29754   108    41]\n",
      " [   54    18 29441   487]\n",
      " [   85    14  1271 28630]]\n",
      "\n",
      "Test Set:\n",
      "Accuracy: 0.901578947368421\n",
      "Precision: 0.9033446784072748\n",
      "Recall: 0.901578947368421\n",
      "F1 Score: 0.9020756205379947\n",
      "Confusion Matrix: [[1690   24   95   91]\n",
      " [  21 1827   33   19]\n",
      " [  43    7 1697  153]\n",
      " [  49   14  199 1638]]\n"
     ]
    }
   ],
   "source": [
    "train_data=pd.read_csv('./train.csv')\n",
    "sentences_train = preprocess_text(train_data['Description'])\n",
    "sg_model = SkipGramNegativeSampling(sentences_train,window_size=10)\n",
    "sg_model.train(epochs=10)\n",
    "word_vectors_sg=sg_model.W\n",
    "word_index_sg=sg_model.word2id\n",
    "\n",
    "test_data = pd.read_csv('./test.csv')\n",
    "\n",
    "sentences_train= preprocess_text(train_data['Description'])\n",
    "sentences_test = preprocess_text(test_data['Description'],'test')\n",
    "\n",
    "sentence_lengths = [len(sentence) for sentence in sentences_train]\n",
    "sorted_lengths = sorted(sentence_lengths)\n",
    "index_95th_percentile = int(np.percentile(range(len(sorted_lengths)), 95))\n",
    "length_95th_percentile = sorted_lengths[index_95th_percentile]\n",
    "length_sentence = length_95th_percentile\n",
    "\n",
    "X_train = []\n",
    "for sentence in sentences_train:\n",
    "    sentence_embedding = [word_vectors_sg[word_index_sg[word]] for word in sentence]\n",
    "    if len(sentence) < length_sentence:\n",
    "        padding_needed = length_sentence - len(sentence)\n",
    "        sentence_embedding.extend(padding_needed*[word_vectors_sg[word_index_sg[PAD_TOKEN]]])\n",
    "    if sentence_embedding:\n",
    "        X_train.append((sentence_embedding[:length_sentence]))\n",
    "y_train = pd.get_dummies(train_data['Class Index'], prefix='value', dtype=int).values\n",
    "\n",
    "X_test = []\n",
    "for sentence in sentences_test:\n",
    "    sentence_embedding = [word_vectors_sg[word_index_sg.get(word, word_index_sg[UNKNOWN_TOKEN])] for word in sentence]\n",
    "    if len(sentence) < length_sentence:\n",
    "        padding_needed = length_sentence - len(sentence)\n",
    "        sentence_embedding.extend(padding_needed*[word_vectors_sg[word_index_sg[PAD_TOKEN]]])\n",
    "    if sentence_embedding:\n",
    "        X_test.append((sentence_embedding[:length_sentence]))\n",
    "y_test = pd.get_dummies(test_data['Class Index'], prefix='value', dtype=int).values\n",
    "\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.float32)\n",
    "\n",
    "class RNNClassifier(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(RNNClassifier, self).__init__()\n",
    "        self.rnn = nn.LSTM(input_size, hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        output, _ = self.rnn(x)\n",
    "        output = self.fc(output[:, -1, :]) \n",
    "        return output\n",
    "    \n",
    "input_size = 300  \n",
    "hidden_size = 128\n",
    "output_size = 4\n",
    "model = RNNClassifier(input_size, hidden_size, output_size)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "epochs = 10\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    for batch_X, batch_y in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(batch_X)\n",
    "        loss = criterion(outputs, batch_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(f\"Epoch {epoch+1}/{epochs}, Loss: {loss.item()}\")\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    correct_samples=0\n",
    "    total_samples=0\n",
    "    for batch_X, batch_y in train_loader:\n",
    "        outputs = model(batch_X)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        y_true.extend(torch.argmax(batch_y, dim=1).numpy())\n",
    "        y_pred.extend(predicted.numpy())\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    precision = precision_score(y_true, y_pred, average='weighted')\n",
    "    recall = recall_score(y_true, y_pred, average='weighted')\n",
    "    f1 = f1_score(y_true, y_pred, average='weighted')\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    print(\"Train Set:\")\n",
    "    print(\"Accuracy:\",accuracy)\n",
    "    print(\"Precision:\", precision)\n",
    "    print(\"Recall:\",recall)\n",
    "    print(\"F1 Score:\",f1)\n",
    "    print(\"Confusion Matrix:\",cm)\n",
    "    print()\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    outputs = model(X_test_tensor)\n",
    "    _, predicted = torch.max(outputs, 1)\n",
    "    y_true=torch.argmax(y_test_tensor, dim=1).numpy()\n",
    "    y_pred=predicted.numpy()\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    precision = precision_score(y_true, y_pred, average='weighted')\n",
    "    recall = recall_score(y_true, y_pred, average='weighted')\n",
    "    f1 = f1_score(y_true, y_pred, average='weighted')\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    print(\"Test Set:\")\n",
    "    print(\"Accuracy:\",accuracy)\n",
    "    print(\"Precision:\", precision)\n",
    "    print(\"Recall:\",recall)\n",
    "    print(\"F1 Score:\",f1)\n",
    "    print(\"Confusion Matrix:\",cm)\n",
    "\n",
    "table.add_row([5,accuracy,precision,recall,f1,cm])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------+--------------------+--------------------+--------------------+--------------------+-------------------------+\n",
      "| Context Window Size |      Accuracy      |     Precision      |       Recall       |      F1_Score      |     Confusion Matrix    |\n",
      "+---------------------+--------------------+--------------------+--------------------+--------------------+-------------------------+\n",
      "|          2          | 0.8953947368421052 | 0.8969339297630881 | 0.8953947368421052 | 0.8952788674747271 |  [[1701   45   54  100] |\n",
      "|                     |                    |                    |                    |                    |   [  26 1835   14   25] |\n",
      "|                     |                    |                    |                    |                    |   [  82   21 1560  237] |\n",
      "|                     |                    |                    |                    |                    |  [  58   28  105 1709]] |\n",
      "|          5          | 0.901578947368421  | 0.9033446784072748 | 0.901578947368421  | 0.9020756205379947 |  [[1690   24   95   91] |\n",
      "|                     |                    |                    |                    |                    |   [  21 1827   33   19] |\n",
      "|                     |                    |                    |                    |                    |   [  43    7 1697  153] |\n",
      "|                     |                    |                    |                    |                    |  [  49   14  199 1638]] |\n",
      "|          10         | 0.9073684210526316 | 0.9094448803242379 | 0.9073684210526316 | 0.907279656801976  |  [[1719   35   61   85] |\n",
      "|                     |                    |                    |                    |                    |   [  22 1842   19   17] |\n",
      "|                     |                    |                    |                    |                    |   [  75   12 1570  243] |\n",
      "|                     |                    |                    |                    |                    |  [  47   12   76 1765]] |\n",
      "+---------------------+--------------------+--------------------+--------------------+--------------------+-------------------------+\n"
     ]
    }
   ],
   "source": [
    "print(table)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
